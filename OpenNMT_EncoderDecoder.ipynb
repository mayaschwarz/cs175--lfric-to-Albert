{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "OpenNMT EncoderDecoder.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "L_GR9iUHfzL9",
        "5rBaibtnSnz0",
        "FPrbGwuRQQJF",
        "jUZIeeDNlJJD"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7QLIgUmO4gf"
      },
      "source": [
        "# OpenNMT-py Encoder Decoder LSTM\r\n",
        "\r\n",
        "Sequence-to-Sequence Encoder-Decoder Models for translating Middle, Modern, and Old English.\r\n",
        "\r\n",
        "We implemented an Encoder Decoder architecture with an Attention Mechanism, multiple layers, and bidirectional encoding.\r\n",
        "\r\n",
        "We were running into difficulties implementing beam search for decoding our custom model, eventually deciding to utilize the OpenNMT-py framework. The framework provided scripts that would generate, train, and translate a model given a configuration script and data. \r\n",
        "\r\n",
        "It also allowed for our smallest dataset (Old English) to double in size from (~3k to ~5k) sentence pairs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_GR9iUHfzL9"
      },
      "source": [
        "## Google Colab Set Up\r\n",
        "\r\n",
        "Steps that need to be taken to set up the Google Colab Environment.If you're running this locally, feel free to ignore this section. \r\n",
        "\r\n",
        "The only requirement is that you must install the required packages using `requirements.txt`. If there is any dependency errors, please raise an Issue with the repository."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbRUvKr9nywe"
      },
      "source": [
        "from google.colab import drive\r\n",
        "\r\n",
        "# default location for the drive\r\n",
        "ROOT = \"/content/gdrive\"\r\n",
        "\r\n",
        "drive.mount(ROOT)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmTJLiqgn1X1"
      },
      "source": [
        "# Clone github repository setup\r\n",
        "# import join used to join ROOT path and MY_GOOGLE_DRIVE_PATH\r\n",
        "from os.path import join  \r\n",
        "\r\n",
        "# path to your project on Google Drive\r\n",
        "MY_GOOGLE_DRIVE_PATH = 'My Drive/cs175-Aelfric-to-Albert' \r\n",
        "GIT_USERNAME = \"mayaschwarz\" \r\n",
        "\r\n",
        "# Put your Token here! Do not save to the repo with it!\r\n",
        "GIT_TOKEN_PATH = join(ROOT, \"Shareddrives/CS 175 Project/token.txt\")\r\n",
        "GIT_TOKEN = \"\"\r\n",
        "\r\n",
        "with open(GIT_TOKEN_PATH, 'r') as f:\r\n",
        "  GIT_TOKEN = f.readline().strip()\r\n",
        "\r\n",
        "if not GIT_TOKEN:\r\n",
        "  raise ValueError(\"GIT_TOKEN MISSING\")\r\n",
        "\r\n",
        "GIT_REPOSITORY = \"cs175--lfric-to-Albert\" \r\n",
        "\r\n",
        "PROJECT_PATH = join(ROOT, MY_GOOGLE_DRIVE_PATH)\r\n",
        "\r\n",
        "# It's good to print out the value if you are not sure \r\n",
        "print(\"PROJECT_PATH: \", PROJECT_PATH)   \r\n",
        "\r\n",
        "GIT_PATH = \"https://github.com/mayaschwarz/cs175--lfric-to-Albert.git\"\r\n",
        "print(\"GIT_PATH: \", GIT_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MsXX3Smn2hs"
      },
      "source": [
        "# Answer input query for downloading git repository\r\n",
        "while True:\r\n",
        "    response = input(\"Are you sure you want to download the repo? Doing so will delete all unpush work. [y|N] \").lower().strip()\r\n",
        "    if not response or response[0] == 'n':\r\n",
        "        break\r\n",
        "    elif response[0] == \"y\":\r\n",
        "        !rm -rv \"{PROJECT_PATH}\"\r\n",
        "        !mkdir -p \"{PROJECT_PATH}\" \r\n",
        "        !git clone \"{GIT_PATH}\" \"{PROJECT_PATH}\"\r\n",
        "        break\r\n",
        "\r\n",
        "# cd into the repository\r\n",
        "%cd \"{PROJECT_PATH}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpBwKtztn31i"
      },
      "source": [
        "# Check that repository is up to date\r\n",
        "!git pull "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leBL9jCcJ6cK"
      },
      "source": [
        "# Check which branch you're on\r\n",
        "!git branch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3g-49GKrohA"
      },
      "source": [
        "# On Google Colab ONLY\r\n",
        "# Reinstall Torch to avoid incompatibility with Cuda 10.1\r\n",
        "\r\n",
        "# NOTE: By the end of the insatallation, it might ask for restarting the runtime...\r\n",
        "# In this case, just click the \"RESTART RUNTIME\" button.\r\n",
        "!pip install --ignore-installed torch==1.6.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html > /dev/null\r\n",
        "!pip install git+https://github.com/huggingface/datasets.git@master > /dev/null\r\n",
        "\r\n",
        "!pip install contractions > /dev/null\r\n",
        "!pip install cltk==0.1.121 > /dev/null\r\n",
        "!pip install nltk==3.5 > /dev/null\r\n",
        "!pip install pyyaml==5.3.1 > /dev/null\r\n",
        "!pip install sacrebleu > /dev/null\r\n",
        "!pip install torchvision==0.7.0 > /dev/null\r\n",
        "!pip install OpenNMT-py > /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qA090PA8PLeH"
      },
      "source": [
        "## Setting up the Python Environment\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJPEpWrpEugD"
      },
      "source": [
        "# load notebook environment variables\r\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3pyyJYjoGwV"
      },
      "source": [
        "# standard library\r\n",
        "import math\r\n",
        "from os import listdir\r\n",
        "import re\r\n",
        "import random\r\n",
        "\r\n",
        "# additional libraries (pip install ..)\r\n",
        "import cltk\r\n",
        "import nltk\r\n",
        "import onmt\r\n",
        "from onmt.utils.misc import set_random_seed\r\n",
        "import pyonmttok\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "from torchtext.data import Dataset\r\n",
        "import yaml\r\n",
        "\r\n",
        "# local libraries\r\n",
        "from src.data_manager import *\r\n",
        "from src.paths import *"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqRuGHcgpLPX"
      },
      "source": [
        "def set_deterministic(seed: int = 1234):\r\n",
        "    random.seed(seed)\r\n",
        "    torch.manual_seed(seed)\r\n",
        "    torch.cuda.manual_seed(seed)\r\n",
        "    torch.backends.cudnn.deterministic = True\r\n",
        "    set_random_seed(seed, torch.cuda.is_available())\r\n",
        "\r\n",
        "set_deterministic()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FJ8_RlhiU_N"
      },
      "source": [
        "## Preprocessing and Tokenization\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWn1nklMSqHm"
      },
      "source": [
        "## Data Retrieval\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Fokf_1WSxt0"
      },
      "source": [
        "from pandas import read_csv\r\n",
        "\r\n",
        "class HomiliesDataset(Dataset):\r\n",
        "    '''\r\n",
        "    Processes the Homilies Dataset\r\n",
        "\r\n",
        "    Arguments:\r\n",
        "      path{str|Path} -- path to the filename containing the homilies dataset\r\n",
        "      rever\r\n",
        "    '''\r\n",
        "    def  __init__(self, path, reverse=False):\r\n",
        "        df = read_csv (path)\r\n",
        "        self.src = list(df['text'])\r\n",
        "        self.src_key = 't_old'\r\n",
        "        self.tgt = list(df['translation'])\r\n",
        "        self.tgt_key = 't_mod'\r\n",
        "\r\n",
        "        if reverse:\r\n",
        "            self.src, self.tgt, self.src_key = self.tgt, self.src\r\n",
        "            self.src_key, self.tgt_key = self.tgt_key, self.src_key\r\n",
        "\r\n",
        "    def __getitem__(self, index):\r\n",
        "        return self.src[index], self.tgt[index]\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.src)\r\n",
        "\r\n",
        "    def bible_format(self, training: float = 0.7, valid: float = 0.0) -> {str : {str : [str]}}:\r\n",
        "        '''\r\n",
        "        Returns the dataset formatted similar to the bible datasets to allow\r\n",
        "        common operations.\r\n",
        "\r\n",
        "        Keys are accessed as t_old and t_mod to access the versions\r\n",
        "\r\n",
        "        Arguments:\r\n",
        "            training{float} -- perentage of training data\r\n",
        "            valid{float} -- percentage of data set aside for validation, rest is test data\r\n",
        "        '''\r\n",
        "        dataset = { 'training': { self.src_key : [], self.tgt_key: []}, \r\n",
        "                    'validation': { self.src_key : [], self.tgt_key: []}, \r\n",
        "                    'test': { self.src_key : [], self.tgt_key: []} \r\n",
        "                  }\r\n",
        "\r\n",
        "        n = len(self)\r\n",
        "        train_size = int(training * n)\r\n",
        "        valid_size = int(valid * n)\r\n",
        "        test_size  = n - train_size - valid_size\r\n",
        "        train, valid, test = torch.utils.data.random_split(\r\n",
        "                                                          self, \r\n",
        "                                                          [\r\n",
        "                                                           train_size, \r\n",
        "                                                           valid_size, \r\n",
        "                                                           test_size\r\n",
        "                                                           ])\r\n",
        "        src_train, tgt_train = zip(*train)\r\n",
        "        src_valid, tgt_valid = zip(*valid)\r\n",
        "        src_test, tgt_test = zip(*test)\r\n",
        "\r\n",
        "        dataset['training'][self.src_key] = list(src_train)\r\n",
        "        dataset['training'][self.tgt_key] = list(tgt_train)\r\n",
        "\r\n",
        "        dataset['validation'][self.src_key] = list(src_valid)\r\n",
        "        dataset['validation'][self.tgt_key] = list(tgt_valid)\r\n",
        "\r\n",
        "        dataset['test'][self.src_key] = list(src_test)\r\n",
        "        dataset['test'][self.tgt_key] = list(tgt_test)\r\n",
        "\r\n",
        "        return dataset"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rBaibtnSnz0"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLmGH83poeo4"
      },
      "source": [
        "import cltk\r\n",
        "from cltk.corpus.middle_english.alphabet import normalize_middle_english\r\n",
        "from cltk.phonology.old_english.phonology import Word\r\n",
        "from typing import Union\r\n",
        "\r\n",
        "def _normalize(text: str, language_code: str):\r\n",
        "    if language_code == 'ang':\r\n",
        "        # old english\r\n",
        "        DONT_NORMALIZE = '!?.&,:;\"'\r\n",
        "        normalized_words = list()\r\n",
        "        for word in text.split():\r\n",
        "            if len(word) == 0:\r\n",
        "                continue\r\n",
        "\r\n",
        "            if word[-1] in DONT_NORMALIZE:\r\n",
        "                normalized_words.append(Word(word[:-1]).ascii_encoding() + word[-1])\r\n",
        "            else:\r\n",
        "                normalized_words.append(Word(word).ascii_encoding())\r\n",
        "\r\n",
        "        return ' '.join(normalized_words)\r\n",
        "    elif language_code == 'enm':\r\n",
        "        # middle english\r\n",
        "        return normalize_middle_english(text, to_lower=False, alpha_conv=True, punct=False)\r\n",
        "    return text\r\n",
        "\r\n",
        "def tokenizer(text: str, language_code: str, **kwargs: bool) -> [str]:\r\n",
        "    tok = pyonmttok.Tokenizer(\"aggressive\", joiner_annotate=True, **kwargs)\r\n",
        "    tokens, _ = tok.tokenize(_normalize(text, language_code))\r\n",
        "    return tokens\r\n",
        "\r\n",
        "def write_tokenized_dataset(dataset: {str: {str: [str]}}, source: str, source_language_code: str, target: str, target_language_code: str, file_paths: {str, Union[str, Path], Union[str, Path]}, token_kwargs: {str: bool} = {}) -> None:\r\n",
        "    \"\"\"\r\n",
        "    Given a dataset, tokenizes and writes the contents according to it's file path\r\n",
        "\r\n",
        "    Arguments:\r\n",
        "      dataset {{str: [str]}} -- dataset returned from create_datasets\r\n",
        "      file_paths - dictionary with key as the dataset-type (training, validation, test), item as (path to source, path to target)\r\n",
        "      token_kwargs {{str: bool}} -- kwargs for the tokenizer (case_markup, etc.)\r\n",
        "    \"\"\"\r\n",
        "    for dataset_t in file_paths.keys():\r\n",
        "        src_path, tgt_path = file_paths[dataset_t]\r\n",
        "        with open(src_path, mode='w+', encoding='utf-8') as src, open(tgt_path, mode='w+', encoding='utf-8') as tgt:\r\n",
        "            src.write('\\n'.join([\" \".join(tokenizer(l, source_language_code, **token_kwargs)) for l in dataset[dataset_t][source]]))\r\n",
        "            tgt.write('\\n'.join([\" \".join(tokenizer(l, target_language_code, **token_kwargs)) for l in dataset[dataset_t][target]]))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPrbGwuRQQJF"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDxEfQtt20T7",
        "outputId": "c7a546da-06f3-42f8-e4d5-53d4ae7120e1"
      },
      "source": [
        "# Check if GPU is active\r\n",
        "# If not, go to \"Runtime\" menu > \"Change runtime type\" > \"GPU\"\r\n",
        "\r\n",
        "!nvidia-smi -L"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU 0: Tesla P100-PCIE-16GB (UUID: GPU-5e4b5940-c5d2-fe17-5ce0-e30d01cf8f2f)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmW12nkB21K-",
        "outputId": "78817373-b809-4ed4-d754-3c1630199033"
      },
      "source": [
        "# Make sure the GPU is visable to PyTorch\r\n",
        "import torch\r\n",
        "\r\n",
        "gpu_id = torch.cuda.current_device()\r\n",
        "print(torch.cuda.is_available())\r\n",
        "print(torch.cuda.get_device_name(gpu_id))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1Bd9FKIg-Ml"
      },
      "source": [
        "def build_and_train(config_path):\r\n",
        "    # build and store vocab in run folder\r\n",
        "    !onmt_build_vocab -config \"{config_path}\" -n_sample -1\r\n",
        "    # begin training\r\n",
        "    !onmt_train -config \"{config_path}\""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUZIeeDNlJJD"
      },
      "source": [
        "# Translation and Evaluation\r\n",
        "\r\n",
        "See [here](https://opennmt.net/OpenNMT-py/options/translate.html) for more info on translation parameters\r\n",
        "\r\n",
        "Evaluatation using BLEU and METEOR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwt4xA_EnMmG"
      },
      "source": [
        "from datasets import list_metrics, load_metric\r\n",
        "from nltk.translate.bleu_score import sentence_bleu\r\n",
        "from nltk.translate import meteor\r\n",
        "\r\n",
        "def compute_score(candidate_verses: [str], reference_verses: [str], metric_name: str = 'sacrebleu') -> float:\r\n",
        "    metric = load_metric(metric_name)\r\n",
        "    # if it's sacrebleu, need to reformat\r\n",
        "    if metric_name == 'sacrebleu':\r\n",
        "        reference_verses = [[r] for r in reference_verses]\r\n",
        "    \r\n",
        "    if len(candidate_verses) < len(reference_verses):\r\n",
        "        print(\"candidate verses is less than reference verses, trimming reference to fit\")\r\n",
        "        reference_verses = reference_verses[:-1]\r\n",
        "\r\n",
        "    metric.add_batch(predictions = candidate_verses, references = reference_verses)\r\n",
        "    \r\n",
        "    return metric.compute()\r\n",
        "\r\n",
        "def get_detokenized_file(filename: Union[str, Path], tokenize: pyonmttok.Tokenizer) -> [str]:\r\n",
        "    with open(filename, encoding='utf-8') as f:\r\n",
        "        # Add line to strip empty lines\r\n",
        "        lines = [tokenize.detokenize(line.rstrip('\\n').split(' ')) for line in f]\r\n",
        "        if lines[-1] == '':\r\n",
        "            lines = lines[:-1]\r\n",
        "\r\n",
        "        return lines\r\n",
        "\r\n",
        "def get_score(metric_name: str, value: {str: float}) -> None:\r\n",
        "    if metric_name == 'sacrebleu':\r\n",
        "        return value['score']\r\n",
        "    elif metric_name == 'meteor':\r\n",
        "        return value['meteor']\r\n",
        "\r\n",
        "def evaluate(models: Union[str, Path], source: Union[str, Path], target: Union[str, Path], eval_metrics: [str], token_kwargs: {str:bool},  max_length: int, beam_size: 5, save_folder='./predictions', verbose=True) -> {str: {str: {}}}:\r\n",
        "    tokenize = pyonmttok.Tokenizer(\"aggressive\", **token_kwargs)\r\n",
        "    # detokenize the reference file that has been tokenized\r\n",
        "    # (this ensures that any normalization techniques used do not effect the scoring)\r\n",
        "    references = get_detokenized_file(target, tokenize)\r\n",
        "  \r\n",
        "    scores = dict() \r\n",
        "    for m in models:\r\n",
        "        # get the model name\r\n",
        "        model_name = m.name[:-3] if isinstance(m, Path) else m.rsplit('(\\\\|\\/)')[-1][:-3]\r\n",
        "\r\n",
        "        filename = f\"{save_folder}/{model_name}_pred.txt\"\r\n",
        "        \r\n",
        "        # Call the translate script to generate token predictions\r\n",
        "        !onmt_translate -model \"{m}\" -src \"{source}\" -output \"{filename}\" -min_length 1 -max_length \"{max_length}\" -beam_size \"{beam_size}\" -gpu 0 \r\n",
        "        \r\n",
        "        # Retrieve candidate sentences\r\n",
        "        candidates = get_detokenized_file(filename, tokenize)\r\n",
        "        \r\n",
        "        print(f'{model_name} SCORE:')\r\n",
        "\r\n",
        "        metrics = dict()\r\n",
        "        for eval_name in eval_metrics:\r\n",
        "            eval_score = compute_score(candidates, references, eval_name)\r\n",
        "            if verbose:\r\n",
        "                print(f'\\t{eval_name} = {get_score(eval_name, eval_score):.4f}')\r\n",
        "            metrics[eval_name] = eval_score\r\n",
        "        scores[model_name] = metrics\r\n",
        "\r\n",
        "    return scores"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHfn46hoA8bw"
      },
      "source": [
        "# Configuring the Data, Model, and Training Parameters\r\n",
        "Generate a YAML file that contains all the hyperparameters and system variables necessary to build the vocab, build, and train the model.\r\n",
        "\r\n",
        "See [here](https://opennmt.net/OpenNMT-py/options/build_vocab.html) for more info on building vocab\r\n",
        "\r\n",
        "See [here](https://opennmt.net/OpenNMT-py/options/train.html) for more info about building the model and training parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1OT26GbF_Tj"
      },
      "source": [
        "# declare the config folder to store all the yaml files\r\n",
        "CONFIG_NAME = 'openmt-config'\r\n",
        "!mkdir -p \"{CONFIG_NAME}\"\r\n",
        "CONFIG_PATH = Path(CONFIG_NAME)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqaPzHBpCpZG"
      },
      "source": [
        "## Middle and Modern English"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6G-C-dD0EoHQ"
      },
      "source": [
        "### Middle to Modern\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWdU14UyBZIi"
      },
      "source": [
        "from pathlib import Path\r\n",
        "\r\n",
        "ENM2MOD_TRANSLATE_NAME = 'enm2mod'\r\n",
        "!mkdir -p '{ENM2MOD_TRANSLATE_NAME}'\r\n",
        "\r\n",
        "# PATH VARIABLES\r\n",
        "ENM2MOD_TRANSLATE_PATH = Path(ENM2MOD_TRANSLATE_NAME)\r\n",
        "ENM2MOD_RUN_PATH = ENM2MOD_TRANSLATE_PATH / 'run'\r\n",
        "!mkdir -p \"{ENM2MOD_RUN_PATH}\"\r\n",
        "\r\n",
        "# Dataset Variables\r\n",
        "ENM2MOD_SOURCE_VER = 't_wyc'\r\n",
        "ENM2MOD_SRC_LANG_CODE = 'enm'\r\n",
        "ENM2MOD_TARGET_VER = 't_kjv'\r\n",
        "ENM2MOD_TGT_LANG_CODE = 'eng'\r\n",
        "\r\n",
        "MAX_SENTENCE_LENGTH = 60\r\n",
        "\r\n",
        "# Dataset Paths\r\n",
        "DATA_PATH = Path('data/preprocessed')\r\n",
        "!mkdir -p \"{DATA_PATH}\"\r\n",
        "\r\n",
        "set_deterministic()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzd-aZpnBczZ"
      },
      "source": [
        "# Generate splits and write to files\r\n",
        "versions = get_bible_versions_by_file_name([ENM2MOD_SOURCE_VER, ENM2MOD_TARGET_VER])\r\n",
        "\r\n",
        "datasets = create_datasets(versions, .82, \r\n",
        "                preprocess_operations = [preprocess_filter_num_words(MAX_SENTENCE_LENGTH),\r\n",
        "                                         preprocess_expand_contractions(),\r\n",
        "                                         preprocess_filter_num_sentences(),\r\n",
        "                ]);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Elj5pmCtBN7X"
      },
      "source": [
        "ENM2MOD_SRC_EXT = ENM2MOD_SOURCE_VER[2:]\r\n",
        "ENM2MOD_TGT_EXT = ENM2MOD_TARGET_VER[2:]\r\n",
        "\r\n",
        "\r\n",
        "enm2mod_file_paths = {\r\n",
        "    'training' : (DATA_PATH / f'bible-train.{ENM2MOD_SRC_EXT}', DATA_PATH / f'bible-train.{ENM2MOD_TGT_EXT}'),\r\n",
        "    'validation' : (DATA_PATH / f'bible-valid.{ENM2MOD_SRC_EXT}', DATA_PATH / f'bible-valid.{ENM2MOD_TGT_EXT}'),\r\n",
        "    'test' : (DATA_PATH / f'bible-test.{ENM2MOD_SRC_EXT}', DATA_PATH / f'bible-test.{ENM2MOD_TGT_EXT}')\r\n",
        "    }\r\n",
        "\r\n",
        "token_kwargs = {\r\n",
        "    'case_markup': True\r\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoqCvH6iogUc"
      },
      "source": [
        "write_tokenized_dataset(datasets, ENM2MOD_SOURCE_VER, ENM2MOD_SRC_LANG_CODE, ENM2MOD_TARGET_VER, ENM2MOD_TGT_LANG_CODE, enm2mod_file_paths, token_kwargs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TG5VQ6ZPA2yV"
      },
      "source": [
        "ENM2MOD_SRC_VOCAB_PATH = ENM2MOD_RUN_PATH / 'vocab.src'\r\n",
        "ENM2MOD_TGT_VOCAB_PATH = ENM2MOD_RUN_PATH / 'vocab.tgt'\r\n",
        "\r\n",
        "enm2mod_yaml = 'enm2mod.yaml'\r\n",
        "\r\n",
        "ENM2MOD_MODEL_PATH = ENM2MOD_RUN_PATH / 'models'\r\n",
        "ENM2MOD_MODEL_PREFIX = 'enm2mod'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaTEGa-fAzej"
      },
      "source": [
        "config =  f'''# {enm2mod_yaml}\r\n",
        "save_data: {ENM2MOD_RUN_PATH}\r\n",
        "\r\n",
        "### DATA PROPROCESSING ###\r\n",
        "## Where the vocab(s) will be written\r\n",
        "src_vocab: {ENM2MOD_SRC_VOCAB_PATH}\r\n",
        "tgt_vocab: {ENM2MOD_TGT_VOCAB_PATH}\r\n",
        "\r\n",
        "# Corpus opts:\r\n",
        "data:\r\n",
        "    corpus_1:\r\n",
        "        path_src: {enm2mod_file_paths['training'][0]}\r\n",
        "        path_tgt: {enm2mod_file_paths['training'][1]}\r\n",
        "        transforms: []\r\n",
        "        weight: 1\r\n",
        "    valid:\r\n",
        "        path_src: {enm2mod_file_paths['validation'][0]}\r\n",
        "        path_tgt: {enm2mod_file_paths['validation'][1]}\r\n",
        "        transforms: []\r\n",
        "\r\n",
        "## silently ignore empty lines in data\r\n",
        "skip_empty_level: silent\r\n",
        "\r\n",
        "### TRAINING ###\r\n",
        "## Where the model will be saved\r\n",
        "save_model: {ENM2MOD_MODEL_PATH / ENM2MOD_MODEL_PREFIX}\r\n",
        "save_checkpoint_steps: 1000\r\n",
        "average_decay: 0.0005\r\n",
        "seed: 1234\r\n",
        "report_every: 100\r\n",
        "train_steps: 100000\r\n",
        "valid_steps: 100\r\n",
        "early_stopping: 10\r\n",
        "# early_stopping_criteria: accuracy\r\n",
        "tensorboard: True\r\n",
        "tensorboard_log_dir: {ENM2MOD_RUN_PATH / 'logs'}\r\n",
        "\r\n",
        "# Batching\r\n",
        "world_size: 1\r\n",
        "gpu_ranks: [0]\r\n",
        "batch_size: 64\r\n",
        "valid_batch_size: 64\r\n",
        "batch_size_multiple: 1\r\n",
        "\r\n",
        "# Optimization\r\n",
        "model_dtype: \"fp32\"\r\n",
        "optim: \"adam\"\r\n",
        "learning_rate: 0.001\r\n",
        "\r\n",
        "# Model\r\n",
        "encoder_type: rnn\r\n",
        "decoder_type: rnn\r\n",
        "rnn_type: LSTM\r\n",
        "bidir_edges: True\r\n",
        "enc_layers: 2\r\n",
        "dec_layers: 2\r\n",
        "rnn_size: 1024\r\n",
        "word_vec_size: 256\r\n",
        "dropout: 0.5\r\n",
        "attn_dropout: 0.3\r\n",
        "'''\r\n",
        "\r\n",
        "with open(CONFIG_PATH / enm2mod_yaml, \"w+\") as config_yaml:\r\n",
        "  config_yaml.write(config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vw90EslMG2ZP"
      },
      "source": [
        "build_and_train(CONFIG_PATH / enm2mod_yaml)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvDyin8CBtTt"
      },
      "source": [
        "# retrieve the models\r\n",
        "enm2mod_models = [ ENM2MOD_MODEL_PATH / f for f in listdir(ENM2MOD_MODEL_PATH) if f.startswith(ENM2MOD_MODEL_PREFIX)]\r\n",
        "\r\n",
        "ENM2MOD_PREDICTIONS_PATH = ENM2MOD_RUN_PATH / 'predictions'\r\n",
        "!mkdir -p \"{ENM2MOD_PREDICTIONS_PATH}\"\r\n",
        "\r\n",
        "eval_metrics = ['sacrebleu', 'meteor']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKLqd9NOUal4"
      },
      "source": [
        "enm2mod_scores = evaluate(enm2mod_models, \r\n",
        "                          enm2mod_file_paths['test'][0], \r\n",
        "                          enm2mod_file_paths['test'][1], \r\n",
        "                          eval_metrics,\r\n",
        "                          token_kwargs,\r\n",
        "                          MAX_SENTENCE_LENGTH, \r\n",
        "                          5, \r\n",
        "                          str(ENM2MOD_PREDICTIONS_PATH))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-80sPpwGlLvL"
      },
      "source": [
        "The best performing model is after 2900 training iterations with early stopping and beam size 5:\r\n",
        "\r\n",
        "    BLEU   = 26.5572\r\n",
        "    METEOR = 0.4481\r\n",
        "\r\n",
        "Interesting note: Adding a capitalization token and keeping punctuation increased both BLEU and METEOR accuracy by 2-4% compared to lowercase without punctuation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkaOH9VmyJJO"
      },
      "source": [
        "#### User Studies Predictions\r\n",
        "\r\n",
        "Generate predictions for the user studies\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a25utt05mPw3"
      },
      "source": [
        "queries = ['In the bigynnyng God made of nouyt heuene and erthe.', \r\n",
        "           'And Adam clepide the name of his wijf Eue, for sche was the moder of alle men lyuynge.', \r\n",
        "           'And the Lord God clepide Adam, and seide to hym, Where art thou?', \r\n",
        "           'Forsothe the Lord hadde mynde of Noe, and of alle lyuynge beestis, and of alle werk beestis, that weren with hym in the schip; and brouyte a wynd on the erthe.', \r\n",
        "           'And whanne God seiy, that the erthe was corrupt, for ech fleisch ether man hadde corrupt his weie on erthe,', \r\n",
        "           'bi tweyne and bi tweyne, male and female entriden to Noe in to the schip, as the Lord comaundide to Noe.', \r\n",
        "           'And sotheli the watrys yeden and decresiden til to the tenthe monethe, for in the tenthe monethe, in the firste dai of the monethe, the coppis of hillis apperiden.', \r\n",
        "           'And God fillide in the seuenthe dai his werk which he made; and he restide in the seuenthe dai fro al his werk which he hadde maad;', \r\n",
        "           'And the erthe brouyte forth greene erbe and makynge seed bi his kynde, and a tre makynge fruyt, and ech hauynge seed by his kynde. And God seiy that it was good.', \r\n",
        "           'And the Lord dide in that nyyt, as Gedeon axide; and drynesse was in the flees aloone, and deew was in al the erthe.', \r\n",
        "           'Also `the trees spaken to the vyne, Come thou, and comaunde to vs.', \r\n",
        "           'Therfor not Y do synne ayens thee, but thou doist yuel ayens me, and bryngist in batels not iust to me; the Lord, iuge of this dai, deme bitwixe the sones of Israel and bitwixe the sones of Amon.', \r\n",
        "           'The trauel of foolis shal turment hem, that kunnen not go in to the citee.', \r\n",
        "           'And if seuene sithis in the dai he do synne ayens thee, and seuene sithis in the dai he be conuertid to thee, and seie, It forthenkith me, foryyue thou hym.', \r\n",
        "           'and seide, Oneli Y knewe you of alle the kynredis of erthe; therfor Y schal visite on you alle youre wickidnessis.', \r\n",
        "           'And he is heed of the bodi of the chirche; which is the bigynnyng and the firste bigetun of deede men, that he holde the firste dignyte in alle thingis.', \r\n",
        "           'And the foure beestis seiden, Amen. And the foure and twenti eldre men fellen doun on her faces, and worschipiden hym that lyueth in to worldis of worldis.', \r\n",
        "           'He brak at noumbre my teeth; he fedde me with aische.', \r\n",
        "           'And if Sathanas be departid ayens hym silf, hou schal his rewme stonde? For ye seien, that Y caste out feendis in Belsabub.', \r\n",
        "           'coueitouse, hiy of bering, proude, blasfemeris, not obedient to fadir and modir, vnkynde,', \r\n",
        "           'Forsothe God seide, Liytis be maad in the firmament of heuene, and departe tho the dai and niyt; and be tho in to signes, and tymes, and daies, and yeeris;', \r\n",
        "           'Isaac dredde bi a greet astonying; and he wondride more, than it mai be bileued, and seide, Who therfor is he which a while ago brouyte to me huntyng takun, and Y eet of alle thingis bifor that thou camest; and Y blesside him? and he schal be blessid.', \r\n",
        "           'And lo! an aungel of the Lord criede fro heuene, and seide, Abraham! Abraham!', \r\n",
        "           'Sotheli Abraham plauntide a wode in Bersabee, and inwardli clepide there the name of euerlastinge God; and he was an erthetiliere ether a comelynge of the lond of Palestynes in many dayes.',\r\n",
        "           'And he helde forth his hond, and took the swerd to sacrifice his sone.', \r\n",
        "           'Abraham turnede ayen to hise children, and thei yeden to Bersabee to gidere, and he dwellide there.', \r\n",
        "           'And whanne ye weren deed in giltis, and in the prepucie of youre fleisch, he quikenyde togidere you with hym;', \r\n",
        "           'Wymmen, be ye sugetis to youre hosebondis, as it bihoueth in the Lord.', \r\n",
        "           'For he that doith iniurie, schal resseyue that that he dide yuele; and acceptacioun of persoones is not anentis God.', \r\n",
        "           'Aristark, prisoner with me, gretith you wel, and Mark, the cosyn of Barnabas, of whom ye han take maundementis; if he come to you, resseyue ye hym;', \r\n",
        "           'But to God and oure fadir be glorie in to worldis of worldis.'\r\n",
        "           ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubw1aaU6yNYB"
      },
      "source": [
        "# Best Performing Model\r\n",
        "model = ENM2MOD_MODEL_PATH / f'{ENM2MOD_MODEL_PREFIX}_step_2900.pt'\r\n",
        "MIDDLE_TEXT_TOK = DATA_PATH / 'user-studies.enm'\r\n",
        "MIDDLE_TEST_PRED = ENM2MOD_PREDICTIONS_PATH / 'middle-text-pred.txt'\r\n",
        "\r\n",
        "with open(MIDDLE_TEXT_TOK, mode='w+', encoding='utf-8') as f:\r\n",
        "      eval_text = [l.rstrip('\\n') for l in f]\r\n",
        "      f.write('\\n'.join([\" \".join(tokenizer(l, 'enm', **token_kwargs)) for l in queries]))\r\n",
        "\r\n",
        "!onmt_translate -model \"{model}\" -src \"{MIDDLE_TEXT_TOK}\" -output \"{MIDDLE_TEST_PRED}\" -min_length 1 -max_length 60 -beam_size 5 -gpu 0 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgnAspYUysW9"
      },
      "source": [
        "tokenize = pyonmttok.Tokenizer(\"aggressive\", **token_kwargs)\r\n",
        "hypotheses = get_detokenized_file(MIDDLE_TEST_PRED, tokenize)\r\n",
        "\r\n",
        "for hyp in hypotheses:\r\n",
        "    print(hyp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRLR8eZFT4Go"
      },
      "source": [
        "### Modern to Middle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFbUFqBwGiC2"
      },
      "source": [
        "We can reuse the preprocessing files saved from the previous model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qEGECBoEsqx"
      },
      "source": [
        "MOD2ENM_TRANSLATE_NAME = 'mod2enm'\r\n",
        "!mkdir -p '{MOD2ENM_TRANSLATE_NAME}'\r\n",
        "\r\n",
        "# PATH VARIABLES\r\n",
        "MOD2ENM_TRANSLATE_PATH = Path(MOD2ENM_TRANSLATE_NAME)\r\n",
        "MOD2ENM_RUN_PATH = MOD2ENM_TRANSLATE_PATH / 'run'\r\n",
        "!mkdir -p \"{MOD2ENM_RUN_PATH}\"\r\n",
        "\r\n",
        "# Dataset Variables (swap previous run)\r\n",
        "MOD2ENM_SOURCE_VER = 't_kjv'\r\n",
        "MOD2ENM_TARGET_VER = 't_wyc'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPcBIfqUE9dg"
      },
      "source": [
        "MOD2ENM_SRC_EXT = MOD2ENM_SOURCE_VER[2:]\r\n",
        "MOD2ENM_TGT_EXT = MOD2ENM_TARGET_VER[2:]\r\n",
        "\r\n",
        "mod2enm_file_paths = {\r\n",
        "    'training' : (DATA_PATH / f'bible-train.{MOD2ENM_SRC_EXT}', DATA_PATH / f'bible-train.{MOD2ENM_TGT_EXT}'),\r\n",
        "    'validation' : (DATA_PATH / f'bible-valid.{MOD2ENM_SRC_EXT}', DATA_PATH / f'bible-valid.{MOD2ENM_TGT_EXT}'),\r\n",
        "    'test' : (DATA_PATH / f'bible-test.{MOD2ENM_SRC_EXT}', DATA_PATH / f'bible-test.{MOD2ENM_TGT_EXT}')\r\n",
        "    }\r\n",
        "\r\n",
        "token_kwargs = {\r\n",
        "    'case_markup': True\r\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4BpV395Sx2X"
      },
      "source": [
        "# datasets are already tokenized by the first run, no need to do again\r\n",
        "# write_tokenized_dataset(datasets, MOD2ENM_SOURCE_VER, ENM2MOD_TGT_LANG_CODE, MOD2ENM_TARGET_VER, ENM2MOD_SRC_LANG_CODE, mod2enm_file_paths, token_kwargs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsQxvqJMdRn8"
      },
      "source": [
        "MOD2ENM_SRC_VOCAB_PATH = MOD2ENM_RUN_PATH / 'vocab.src'\r\n",
        "MOD2ENM_TGT_VOCAB_PATH = MOD2ENM_RUN_PATH / 'vocab.tgt'\r\n",
        "\r\n",
        "mod2enm_yaml = 'mod2enm.yaml'\r\n",
        "\r\n",
        "MOD2ENM_MODEL_PATH = MOD2ENM_RUN_PATH / 'models'\r\n",
        "MOD2ENM_MODEL_PREFIX = 'mod2enm'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzJIRf0IF0A9"
      },
      "source": [
        "config =  f'''# {mod2enm_yaml}\r\n",
        "save_data: {MOD2ENM_RUN_PATH}\r\n",
        "\r\n",
        "### DATA PROPROCESSING ###\r\n",
        "## Where the vocab(s) will be written\r\n",
        "src_vocab: {MOD2ENM_SRC_VOCAB_PATH}\r\n",
        "tgt_vocab: {MOD2ENM_TGT_VOCAB_PATH}\r\n",
        "\r\n",
        "# Corpus opts:\r\n",
        "data:\r\n",
        "    corpus_1:\r\n",
        "        path_src: {mod2enm_file_paths['training'][0]}\r\n",
        "        path_tgt: {mod2enm_file_paths['training'][1]}\r\n",
        "        transforms: []\r\n",
        "        weight: 1\r\n",
        "    valid:\r\n",
        "        path_src: {mod2enm_file_paths['validation'][0]}\r\n",
        "        path_tgt: {mod2enm_file_paths['validation'][1]}\r\n",
        "        transforms: []\r\n",
        "\r\n",
        "## silently ignore empty lines in data\r\n",
        "skip_empty_level: silent\r\n",
        "\r\n",
        "### TRAINING ###\r\n",
        "## Where the model will be saved\r\n",
        "save_model: {MOD2ENM_MODEL_PATH / MOD2ENM_MODEL_PREFIX}\r\n",
        "save_checkpoint_steps: 1000\r\n",
        "average_decay: 0.0005\r\n",
        "seed: 1234\r\n",
        "report_every: 100\r\n",
        "train_steps: 100000\r\n",
        "valid_steps: 100\r\n",
        "early_stopping: 10\r\n",
        "early_stopping_criteria: accuracy\r\n",
        "tensorboard: True\r\n",
        "tensorboard_log_dir: {MOD2ENM_RUN_PATH / 'logs'}\r\n",
        "\r\n",
        "# Batching\r\n",
        "world_size: 1\r\n",
        "gpu_ranks: [0]\r\n",
        "batch_size: 64\r\n",
        "valid_batch_size: 64\r\n",
        "batch_size_multiple: 1\r\n",
        "\r\n",
        "# Optimization\r\n",
        "model_dtype: \"fp32\"\r\n",
        "optim: \"adam\"\r\n",
        "learning_rate: 0.001\r\n",
        "\r\n",
        "# Model\r\n",
        "encoder_type: rnn\r\n",
        "decoder_type: rnn\r\n",
        "rnn_type: LSTM\r\n",
        "bidir_edges: True\r\n",
        "enc_layers: 2\r\n",
        "dec_layers: 2\r\n",
        "rnn_size: 1024\r\n",
        "word_vec_size: 256\r\n",
        "dropout: 0.5\r\n",
        "attn_dropout: 0.3\r\n",
        "'''\r\n",
        "\r\n",
        "with open(CONFIG_PATH / mod2enm_yaml, \"w+\") as config_yaml:\r\n",
        "  config_yaml.write(config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yfSnvqCG5ny"
      },
      "source": [
        "build_and_train(CONFIG_PATH / mod2enm_yaml)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KR9OVz2kASQU"
      },
      "source": [
        "# retrieve the models\r\n",
        "mod2enm_model_paths = [ MOD2ENM_MODEL_PATH / f for f in listdir(MOD2ENM_MODEL_PATH) if f.startswith(MOD2ENM_MODEL_PREFIX)]\r\n",
        "\r\n",
        "MOD2ENM_PREDICTIONS_PATH = MOD2ENM_RUN_PATH / 'predictions'\r\n",
        "!mkdir -p \"{MOD2ENM_PREDICTIONS_PATH}\"\r\n",
        "\r\n",
        "# Don't use meteor on non-english translations\r\n",
        "eval_metrics = ['sacrebleu', 'meteor']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmySfcAznh7H"
      },
      "source": [
        "mod2enm_scores = evaluate(mod2enm_model_paths, \r\n",
        "                          mod2enm_file_paths['test'][0], \r\n",
        "                          mod2enm_file_paths['test'][1], \r\n",
        "                          eval_metrics,\r\n",
        "                          token_kwargs,\r\n",
        "                          MAX_SENTENCE_LENGTH, \r\n",
        "                          5, \r\n",
        "                          MOD2ENM_PREDICTIONS_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dE0IJbgH3VC7"
      },
      "source": [
        "Using BLEU Scoring, the best performing model is after 3000 training iterations at beam size 5:\r\n",
        "\r\n",
        "    BLEU   = 28.4447\r\n",
        "    METEOR = 0.4577"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgOAA9XAYkIc"
      },
      "source": [
        "## Old and Modern"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7bLM4MoYsG8"
      },
      "source": [
        "### Old to Modern"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-kOLWJvYyn2"
      },
      "source": [
        "from pathlib import Path\r\n",
        "\r\n",
        "ANG2MOD_TRANSLATE_NAME = 'ang2mod'\r\n",
        "!mkdir -p '{ANG2MOD_TRANSLATE_NAME}'\r\n",
        "\r\n",
        "# PATH VARIABLES\r\n",
        "ANG2MOD_TRANSLATE_PATH = Path(ANG2MOD_TRANSLATE_NAME)\r\n",
        "ANG2MOD_RUN_PATH = ANG2MOD_TRANSLATE_PATH / 'run'\r\n",
        "!mkdir -p \"{ANG2MOD_RUN_PATH}\"\r\n",
        "\r\n",
        "## Dataset Variables\r\n",
        "# For the Homilies Dataset\r\n",
        "ANG2MOD_HOM_SOURCE_VER = 't_old'\r\n",
        "ANG2MOD_HOM_SRC_LANG_CODE = 'ang'\r\n",
        "ANG2MOD_HOM_TARGET_VER = 't_mod'\r\n",
        "ANG2MOD_HOM_TGT_LANG_CODE = 'eng'\r\n",
        "\r\n",
        "# For the Bible Dataset\r\n",
        "ANG2MOD_SOURCE_VER = 't_alf_wsg'\r\n",
        "ANG2MOD_SRC_LANG_CODE = 'ang'\r\n",
        "ANG2MOD_TARGET_VER = 't_kjv'\r\n",
        "ANG2MOD_TGT_LANG_CODE = 'eng'\r\n",
        "\r\n",
        "MAX_SENTENCE_LENGTH = 60\r\n",
        "\r\n",
        "# Dataset Paths\r\n",
        "DATA_PATH = Path('data/preprocessed')\r\n",
        "!mkdir -p \"{DATA_PATH}\"\r\n",
        "\r\n",
        "set_deterministic()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HhPnhfjHJgm"
      },
      "source": [
        "homilies_raw = HomiliesDataset(MISC_TEXTS_PATH / 't_hom.csv')\r\n",
        "hom_dataset = homilies_raw.bible_format(training=0.7, valid=0.15)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnDO3eUdke86"
      },
      "source": [
        "print(\"# training verses: \\t\", len(hom_dataset['training']['t_old']))\r\n",
        "print(\"# training verses: \\t\", len(hom_dataset['validation']['t_old']))\r\n",
        "print(\"# training verses: \\t\", len(hom_dataset['test']['t_old']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TSkJm1JHIpA"
      },
      "source": [
        "# Generate splits and write to files\r\n",
        "versions = get_bible_versions_by_file_name([ANG2MOD_SOURCE_VER, ANG2MOD_TARGET_VER])\r\n",
        "\r\n",
        "datasets = create_datasets(versions, 0.8, \r\n",
        "                preprocess_operations = [preprocess_filter_num_words(MAX_SENTENCE_LENGTH),\r\n",
        "                                         preprocess_expand_contractions(),\r\n",
        "                ], write_files=True);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pos70JdrILEh"
      },
      "source": [
        "ANG2MOD_HOM_SRC_EXT = ANG2MOD_HOM_SOURCE_VER[2:]\r\n",
        "ANG2MOD_HOM_TGT_EXT = ANG2MOD_HOM_TARGET_VER[2:]\r\n",
        "\r\n",
        "ang2mod_hom_file_paths = {\r\n",
        "    'training' : (DATA_PATH / f'hom-train.{ANG2MOD_HOM_SRC_EXT}', DATA_PATH / f'hom-train.{ANG2MOD_HOM_TGT_EXT}'),\r\n",
        "    'validation' : (DATA_PATH / f'hom-valid.{ANG2MOD_HOM_SRC_EXT}', DATA_PATH / f'hom-valid.{ANG2MOD_HOM_TGT_EXT}'),\r\n",
        "    'test' : (DATA_PATH / f'hom-test.{ANG2MOD_HOM_SRC_EXT}', DATA_PATH / f'hom-test.{ANG2MOD_HOM_TGT_EXT}')\r\n",
        "    }\r\n",
        "\r\n",
        "ANG2MOD_SRC_EXT = ANG2MOD_SOURCE_VER[2:]\r\n",
        "ANG2MOD_TGT_EXT = ANG2MOD_TARGET_VER[2:]\r\n",
        "\r\n",
        "ang2mod_file_paths = {\r\n",
        "    'training' : (DATA_PATH / f'bible-train.{ANG2MOD_SRC_EXT}', DATA_PATH / f'bible-train.{ANG2MOD_TGT_EXT}'),\r\n",
        "    'validation' : (DATA_PATH / f'bible-valid.{ANG2MOD_SRC_EXT}', DATA_PATH / f'bible-valid.{ANG2MOD_TGT_EXT}'),\r\n",
        "    'test' : (DATA_PATH / f'bible-test.{ANG2MOD_SRC_EXT}', DATA_PATH / f'bible-test.{ANG2MOD_TGT_EXT}')\r\n",
        "    }\r\n",
        "\r\n",
        "token_kwargs = {\r\n",
        "    'case_markup': True\r\n",
        "    }"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Efy6XxQTS9Gh"
      },
      "source": [
        "write_tokenized_dataset(hom_dataset, ANG2MOD_HOM_SOURCE_VER, ANG2MOD_HOM_SRC_LANG_CODE, ANG2MOD_HOM_TARGET_VER, ANG2MOD_HOM_TGT_LANG_CODE, ang2mod_hom_file_paths, token_kwargs)\r\n",
        "write_tokenized_dataset(datasets, ANG2MOD_SOURCE_VER, ANG2MOD_SRC_LANG_CODE, ANG2MOD_TARGET_VER, ANG2MOD_TGT_LANG_CODE, ang2mod_file_paths, token_kwargs)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjiQNv3UKKHc"
      },
      "source": [
        "# Need to combine the validation sets for training\r\n",
        "COMBINED_VALID_SRC = DATA_PATH / f'combined-valid.{ANG2MOD_HOM_SRC_EXT}.{ANG2MOD_SRC_EXT}'\r\n",
        "COMBINED_VALID_TGT = DATA_PATH / f'combined-valid.{ANG2MOD_HOM_TGT_EXT}.{ANG2MOD_TGT_EXT}'\r\n",
        "COMBINED_TEST_SRC = DATA_PATH / f'combined-test.{ANG2MOD_HOM_SRC_EXT}.{ANG2MOD_SRC_EXT}'\r\n",
        "COMBINED_TEST_TGT = DATA_PATH / f'combined-test.{ANG2MOD_HOM_TGT_EXT}.{ANG2MOD_TGT_EXT}'"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFs0vVD1sRB8"
      },
      "source": [
        "with open(COMBINED_VALID_SRC, mode='w+', encoding='utf-8') as f:\r\n",
        "    with open(ang2mod_hom_file_paths['validation'][0], encoding='utf-8') as hom:\r\n",
        "        f.write('\\n'.join([l.rstrip('\\n') for l in hom if l != '\\n']))\r\n",
        "    with open(ang2mod_file_paths['validation'][0], encoding='utf-8') as hom:\r\n",
        "        f.write('\\n'.join([l.rstrip('\\n') for l in hom if l != '\\n']))\r\n",
        "\r\n",
        "with open(COMBINED_VALID_TGT, mode='w+', encoding='utf-8') as f:\r\n",
        "    with open(ang2mod_hom_file_paths['validation'][1], encoding='utf-8') as hom:\r\n",
        "        f.write('\\n'.join([l.rstrip('\\n') for l in hom if l != '\\n']))\r\n",
        "    with open(ang2mod_file_paths['validation'][1], encoding='utf-8') as hom:\r\n",
        "        f.write('\\n'.join([l.rstrip('\\n') for l in hom if l != '\\n']))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaE1zjf0sSF6"
      },
      "source": [
        "with open(COMBINED_TEST_SRC, mode='w+', encoding='utf-8') as f:\r\n",
        "    with open(ang2mod_hom_file_paths['test'][0], encoding='utf-8') as hom:\r\n",
        "        f.write('\\n'.join([l.rstrip('\\n') for l in hom if l != '\\n']))\r\n",
        "    with open(ang2mod_file_paths['test'][0], encoding='utf-8') as hom:\r\n",
        "        f.write('\\n'.join([l.rstrip('\\n') for l in hom if l != '\\n']))\r\n",
        "\r\n",
        "with open(COMBINED_TEST_TGT, mode='w+', encoding='utf-8') as f:\r\n",
        "    with open(ang2mod_hom_file_paths['test'][1], encoding='utf-8') as hom:\r\n",
        "        f.write('\\n'.join([l.rstrip('\\n') for l in hom if l != '\\n']))\r\n",
        "    with open(ang2mod_file_paths['test'][1], encoding='utf-8') as hom:\r\n",
        "        f.write('\\n'.join([l.rstrip('\\n') for l in hom if l != '\\n']))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5UQfa5VIP11"
      },
      "source": [
        "ANG2MOD_SRC_VOCAB_PATH = ANG2MOD_RUN_PATH / 'vocab.src'\r\n",
        "ANG2MOD_TGT_VOCAB_PATH = ANG2MOD_RUN_PATH / 'vocab.tgt'\r\n",
        "\r\n",
        "ang2mod_yaml = 'ang2mod.yaml'\r\n",
        "\r\n",
        "ANG2MOD_MODEL_PATH = ANG2MOD_RUN_PATH / 'models'\r\n",
        "ANG2MOD_MODEL_PREFIX = 'ang2mod'"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXPANEslISd7"
      },
      "source": [
        "config =  f'''# {ang2mod_yaml}\r\n",
        "save_data: {ANG2MOD_RUN_PATH}\r\n",
        "\r\n",
        "### DATA PROPROCESSING ###\r\n",
        "## Where the vocab(s) will be written\r\n",
        "src_vocab: {ANG2MOD_SRC_VOCAB_PATH}\r\n",
        "tgt_vocab: {ANG2MOD_TGT_VOCAB_PATH}\r\n",
        "\r\n",
        "# Corpus opts:\r\n",
        "data:\r\n",
        "    corpus_1:\r\n",
        "        path_src: {ang2mod_hom_file_paths['training'][0]}\r\n",
        "        path_tgt: {ang2mod_hom_file_paths['training'][1]}\r\n",
        "        transforms: [filtertoolong]\r\n",
        "        weight: 1\r\n",
        "    corpus_2:\r\n",
        "       path_src: {ang2mod_file_paths['training'][0]}\r\n",
        "       path_tgt: {ang2mod_file_paths['training'][1]}\r\n",
        "       transforms: [filtertoolong]\r\n",
        "       weight: 1\r\n",
        "    valid:\r\n",
        "        path_src: {COMBINED_VALID_SRC}\r\n",
        "        path_tgt: {COMBINED_VALID_TGT}\r\n",
        "        transforms: [filtertoolong]\r\n",
        "\r\n",
        "## silently ignore empty lines in data\r\n",
        "skip_empty_level: silent\r\n",
        "\r\n",
        "# Data Transformations\r\n",
        "### Filter\r\n",
        "src_seq_length: {MAX_SENTENCE_LENGTH}\r\n",
        "tgt_seq_length: {MAX_SENTENCE_LENGTH}\r\n",
        "\r\n",
        "### TRAINING ###\r\n",
        "## Where the model will be saved\r\n",
        "save_model: {ANG2MOD_MODEL_PATH / ANG2MOD_MODEL_PREFIX}\r\n",
        "save_checkpoint_steps: 1000\r\n",
        "average_decay: 0.0005\r\n",
        "seed: 1234\r\n",
        "report_every: 100\r\n",
        "train_steps: 100000\r\n",
        "valid_steps: 100\r\n",
        "early_stopping: 10\r\n",
        "# early_stopping_criteria: accuracy\r\n",
        "tensorboard: True\r\n",
        "tensorboard_log_dir: {ANG2MOD_RUN_PATH / 'logs'}\r\n",
        "\r\n",
        "# Batching\r\n",
        "world_size: 1\r\n",
        "gpu_ranks: [0]\r\n",
        "batch_size: 40\r\n",
        "valid_batch_size: 40\r\n",
        "batch_size_multiple: 1\r\n",
        "\r\n",
        "# Optimization\r\n",
        "model_dtype: \"fp32\"\r\n",
        "optim: \"adam\"\r\n",
        "learning_rate: 0.001\r\n",
        "\r\n",
        "# Model\r\n",
        "encoder_type: rnn\r\n",
        "decoder_type: rnn\r\n",
        "rnn_type: LSTM\r\n",
        "bidir_edges: True\r\n",
        "enc_layers: 2\r\n",
        "dec_layers: 2\r\n",
        "rnn_size: 512\r\n",
        "word_vec_size: 128\r\n",
        "dropout: 0.6\r\n",
        "attn_dropout: 0.4\r\n",
        "# global_attention: dot\r\n",
        "'''\r\n",
        "\r\n",
        "with open(CONFIG_PATH / ang2mod_yaml, \"w+\") as config_yaml:\r\n",
        "  config_yaml.write(config)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ado19PtLlDh"
      },
      "source": [
        "build_and_train(CONFIG_PATH / ang2mod_yaml)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBVvZeEb-l2W"
      },
      "source": [
        "# retrieve the models\r\n",
        "ang2mod_models = [ ANG2MOD_MODEL_PATH / f for f in listdir(ANG2MOD_MODEL_PATH) if f.startswith(ANG2MOD_MODEL_PREFIX)]\r\n",
        "\r\n",
        "ANG2MOD_PREDICTIONS_PATH = ANG2MOD_RUN_PATH / 'predictions'\r\n",
        "!mkdir -p \"{ANG2MOD_PREDICTIONS_PATH}\"\r\n",
        "\r\n",
        "eval_metrics = ['sacrebleu', 'meteor']"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNwnhwMa-oIY"
      },
      "source": [
        "ang2mod_scores = evaluate(ang2mod_models, \r\n",
        "                          COMBINED_TEST_SRC, \r\n",
        "                          COMBINED_TEST_TGT, \r\n",
        "                          eval_metrics,\r\n",
        "                          token_kwargs,\r\n",
        "                          MAX_SENTENCE_LENGTH, \r\n",
        "                          5, \r\n",
        "                          str(ANG2MOD_PREDICTIONS_PATH))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JpSXB57egV8"
      },
      "source": [
        "The best performing model is after 3000 training iterations with early stopping and beam size 5:\r\n",
        "\r\n",
        "    BLEU   = 16.9989\r\n",
        "    METEOR = 0.3338"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhYghOuQewS1"
      },
      "source": [
        "#### User Studies Predictions\r\n",
        "\r\n",
        "Generate predictions for the user studies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98rqNcsWe8Ps"
      },
      "source": [
        "queries = ['ON angynne gesceop God heofonan & eorðan.', \r\n",
        "           'ða gesceop Adam naman his wife, Eua, ðæt is lif, for ðanðe heo is ealra libbendra modor.', \r\n",
        "           'God clypode ða Adam, & cwæð: Adam, hwær eart ðu.', \r\n",
        "           '& GOD ða gemunde Noes fare & ðæra nytena ðe him midwæron, & asende wind ofer eorðan, & ða wæteruwurdon gewanode.', \r\n",
        "           'ða geseah God ðæt seo eorðe wæs gewemmed, for ðan ðeælc flæsc gewemde his weg ofer eorðan.', \r\n",
        "           'comon to Noe in to ðam arce, swa swa God bebead.', \r\n",
        "           '& ða wæteru toeodan & wanodon of ðone teoðan monð, & onðam teoðan monðe æteowedon ðæra muntacnollas.', \r\n",
        "           '& God ða gefylde on ðone seofoðan dæg his weorc ðe heworhte. & he gereste hine on ðone seofoðan dæg fram eallumðam weorcum ðe he gefremode.', \r\n",
        "           '& seo eorðe forðteah growende wyrta & sæd berende be hyrecynne & treow wæstm wyrcende & gehwilcsæd hæbbende æfter his hiwe; God geseah ða ðæt hit godwæs.', \r\n",
        "           'God cwæð ða soðlice: Beo nu leoht on ðære heofenanfæstnysse, & todælan dæg & nihte, & beon to tacnum& to tidum & to dagum & to gearum.', \r\n",
        "           'Mid ðam ðe he wolde þæt weorc begynnan, ða clypode Godesengel ardlice of heofonum, Abraham; Heandwyrde sona.', \r\n",
        "           '& hys swurd ateah þæt he hyne geoffrode on þa ealdanwisan.'\r\n",
        "           ]"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgSIwONzezcO"
      },
      "source": [
        "model = ANG2MOD_MODEL_PATH / f'{ANG2MOD_MODEL_PREFIX}_step_3000.pt'\r\n",
        "OLD_TEXT_TOK = DATA_PATH / 'user-studies.ang'\r\n",
        "OLD_TEST_PRED = ANG2MOD_PREDICTIONS_PATH / 'old-text-pred.txt'\r\n",
        "\r\n",
        "with open(OLD_TEXT_TOK, mode='w+', encoding='utf-8') as f:\r\n",
        "      eval_text = [l.rstrip('\\n') for l in f]\r\n",
        "      f.write('\\n'.join([\" \".join(tokenizer(l, 'enm', **token_kwargs)) for l in queries]))\r\n",
        "\r\n",
        "!onmt_translate -model \"{model}\" -src \"{OLD_TEXT_TOK}\" -output \"{OLD_TEST_PRED}\" -min_length 1 -max_length \"{MAX_SENTENCE_LENGTH}\" -beam_size 5 -gpu 0 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNG5jadCe70T"
      },
      "source": [
        "tokenize = pyonmttok.Tokenizer(\"aggressive\", **token_kwargs)\r\n",
        "hypotheses = get_detokenized_file(OLD_TEST_PRED, tokenize)\r\n",
        "\r\n",
        "for hyp in hypotheses:\r\n",
        "    print(hyp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGa7Q7FNYxMf"
      },
      "source": [
        "### Modern to Old"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vP2qCNTsYzDe"
      },
      "source": [
        "from pathlib import Path\r\n",
        "\r\n",
        "MOD2ANG_TRANSLATE_NAME = 'mod2ang'\r\n",
        "!mkdir -p '{MOD2ANG_TRANSLATE_NAME}'\r\n",
        "\r\n",
        "# PATH VARIABLES\r\n",
        "MOD2ANG_TRANSLATE_PATH = Path(MOD2ANG_TRANSLATE_NAME)\r\n",
        "MOD2ANG_RUN_PATH = MOD2ANG_TRANSLATE_PATH / 'run'\r\n",
        "!mkdir -p \"{MOD2ANG_RUN_PATH}\"\r\n",
        "\r\n",
        "## Dataset Variables\r\n",
        "# For the Homilies Dataset\r\n",
        "MOD2ANG_HOM_SOURCE_VER = 't_mod'\r\n",
        "MOD2ANG_HOM_SRC_LANG_CODE = 'eng'\r\n",
        "MOD2ANG_HOM_TARGET_VER = 't_old'\r\n",
        "MOD2ANG_HOM_TGT_LANG_CODE = 'ang'\r\n",
        "\r\n",
        "# For the Bible Dataset\r\n",
        "MOD2ANG_SOURCE_VER = 't_kjv'\r\n",
        "MOD2ANG_SRC_LANG_CODE = 'eng'\r\n",
        "MOD2ANG_TARGET_VER = 't_alf_wsg'\r\n",
        "MOD2ANG_TGT_LANG_CODE = 'ang'\r\n",
        "\r\n",
        "MAX_SENTENCE_LENGTH = 60\r\n",
        "\r\n",
        "# Dataset Paths\r\n",
        "DATA_PATH = Path('data/preprocessed')\r\n",
        "!mkdir -p \"{DATA_PATH}\""
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yo9Km425jggi"
      },
      "source": [
        "MOD2ANG_HOM_SRC_EXT = MOD2ANG_HOM_SOURCE_VER[2:]\r\n",
        "MOD2ANG_HOM_TGT_EXT = MOD2ANG_HOM_TARGET_VER[2:]\r\n",
        "\r\n",
        "mod2ang_hom_file_paths = {\r\n",
        "    'training' : (DATA_PATH / f'hom-train.{MOD2ANG_HOM_SRC_EXT}', DATA_PATH / f'hom-train.{MOD2ANG_HOM_TGT_EXT}'),\r\n",
        "    'validation' : (DATA_PATH / f'hom-valid.{MOD2ANG_HOM_SRC_EXT}', DATA_PATH / f'hom-valid.{MOD2ANG_HOM_TGT_EXT}'),\r\n",
        "    'test' : (DATA_PATH / f'hom-test.{MOD2ANG_HOM_SRC_EXT}', DATA_PATH / f'hom-test.{MOD2ANG_HOM_TGT_EXT}')\r\n",
        "    }\r\n",
        "\r\n",
        "MOD2ANG_SRC_EXT = MOD2ANG_SOURCE_VER[2:]\r\n",
        "MOD2ANG_TGT_EXT = MOD2ANG_TARGET_VER[2:]\r\n",
        "\r\n",
        "mod2ang_file_paths = {\r\n",
        "    'training' : (DATA_PATH / f'bible-train.{MOD2ANG_SRC_EXT}', DATA_PATH / f'bible-train.{MOD2ANG_TGT_EXT}'),\r\n",
        "    'validation' : (DATA_PATH / f'bible-valid.{MOD2ANG_SRC_EXT}', DATA_PATH / f'bible-valid.{MOD2ANG_TGT_EXT}'),\r\n",
        "    'test' : (DATA_PATH / f'bible-test.{MOD2ANG_SRC_EXT}', DATA_PATH / f'bible-test.{MOD2ANG_TGT_EXT}')\r\n",
        "    }\r\n",
        "\r\n",
        "token_kwargs = {\r\n",
        "    'case_markup': True\r\n",
        "    }"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlYz3qtej2PB"
      },
      "source": [
        "# Need to combine the validation sets for training\r\n",
        "COMBINED_VALID_SRC = DATA_PATH / f'combined-valid.{MOD2ANG_HOM_SRC_EXT}.{MOD2ANG_SRC_EXT}'\r\n",
        "COMBINED_VALID_TGT = DATA_PATH / f'combined-valid.{MOD2ANG_HOM_TGT_EXT}.{MOD2ANG_TGT_EXT}'\r\n",
        "COMBINED_TEST_SRC = DATA_PATH / f'combined-test.{MOD2ANG_HOM_SRC_EXT}.{MOD2ANG_SRC_EXT}'\r\n",
        "COMBINED_TEST_TGT = DATA_PATH / f'combined-test.{MOD2ANG_HOM_TGT_EXT}.{MOD2ANG_TGT_EXT}'"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pS0__xavkBf5"
      },
      "source": [
        "MOD2ANG_SRC_VOCAB_PATH = MOD2ANG_RUN_PATH / 'vocab.src'\r\n",
        "MOD2ANG_TGT_VOCAB_PATH = MOD2ANG_RUN_PATH / 'vocab.tgt'\r\n",
        "\r\n",
        "mod2ang_yaml = 'mod2ang.yaml'\r\n",
        "\r\n",
        "MOD2ANG_MODEL_PATH = MOD2ANG_RUN_PATH / 'models'\r\n",
        "MOD2ANG_MODEL_PREFIX = 'mod2ang'"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFOHsC9bkJ8S"
      },
      "source": [
        "config =  f'''# {mod2ang_yaml}\r\n",
        "save_data: {MOD2ANG_RUN_PATH}\r\n",
        "\r\n",
        "### DATA PROPROCESSING ###\r\n",
        "## Where the vocab(s) will be written\r\n",
        "src_vocab: {MOD2ANG_SRC_VOCAB_PATH}\r\n",
        "tgt_vocab: {MOD2ANG_TGT_VOCAB_PATH}\r\n",
        "\r\n",
        "# Corpus opts:\r\n",
        "data:\r\n",
        "    corpus_1:\r\n",
        "        path_src: {mod2ang_hom_file_paths['training'][0]}\r\n",
        "        path_tgt: {mod2ang_hom_file_paths['training'][1]}\r\n",
        "        transforms: [filtertoolong]\r\n",
        "        weight: 1\r\n",
        "    corpus_2:\r\n",
        "       path_src: {mod2ang_file_paths['training'][0]}\r\n",
        "       path_tgt: {mod2ang_file_paths['training'][1]}\r\n",
        "       transforms: [filtertoolong]\r\n",
        "       weight: 1\r\n",
        "    valid:\r\n",
        "        path_src: {COMBINED_VALID_SRC}\r\n",
        "        path_tgt: {COMBINED_VALID_TGT}\r\n",
        "        transforms: [filtertoolong]\r\n",
        "\r\n",
        "## silently ignore empty lines in data\r\n",
        "skip_empty_level: silent\r\n",
        "\r\n",
        "# Data Transformations\r\n",
        "### Filter\r\n",
        "src_seq_length: {MAX_SENTENCE_LENGTH}\r\n",
        "tgt_seq_length: {MAX_SENTENCE_LENGTH}\r\n",
        "\r\n",
        "### TRAINING ###\r\n",
        "## Where the model will be saved\r\n",
        "save_model: {MOD2ANG_MODEL_PATH / MOD2ANG_MODEL_PREFIX}\r\n",
        "save_checkpoint_steps: 1000\r\n",
        "average_decay: 0.0005\r\n",
        "seed: 1234\r\n",
        "report_every: 100\r\n",
        "train_steps: 100000\r\n",
        "valid_steps: 100\r\n",
        "early_stopping: 10\r\n",
        "# early_stopping_criteria: accuracy\r\n",
        "tensorboard: True\r\n",
        "tensorboard_log_dir: {MOD2ANG_RUN_PATH / 'logs'}\r\n",
        "\r\n",
        "# Batching\r\n",
        "world_size: 1\r\n",
        "gpu_ranks: [0]\r\n",
        "batch_size: 40\r\n",
        "valid_batch_size: 40\r\n",
        "batch_size_multiple: 1\r\n",
        "\r\n",
        "# Optimization\r\n",
        "model_dtype: \"fp32\"\r\n",
        "optim: \"adam\"\r\n",
        "learning_rate: 0.001\r\n",
        "\r\n",
        "# Model\r\n",
        "encoder_type: rnn\r\n",
        "decoder_type: rnn\r\n",
        "rnn_type: LSTM\r\n",
        "bidir_edges: True\r\n",
        "enc_layers: 2\r\n",
        "dec_layers: 2\r\n",
        "rnn_size: 512\r\n",
        "word_vec_size: 128\r\n",
        "dropout: 0.6\r\n",
        "attn_dropout: 0.4\r\n",
        "'''\r\n",
        "\r\n",
        "with open(CONFIG_PATH / mod2ang_yaml, \"w+\") as config_yaml:\r\n",
        "  config_yaml.write(config)"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdTZFn_Yyr3Z"
      },
      "source": [
        "build_and_train(CONFIG_PATH / mod2ang_yaml)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFt9nbXgmmpj"
      },
      "source": [
        "# retrieve the models\r\n",
        "mod2ang_models = [ MOD2ANG_MODEL_PATH / f for f in listdir(MOD2ANG_MODEL_PATH) if f.startswith(MOD2ANG_MODEL_PREFIX)]\r\n",
        "\r\n",
        "MOD2ANG_PREDICTIONS_PATH = MOD2ANG_RUN_PATH / 'predictions'\r\n",
        "!mkdir -p \"{MOD2ANG_PREDICTIONS_PATH}\"\r\n",
        "\r\n",
        "eval_metrics = ['sacrebleu', 'meteor']"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AG50ke1mx0B"
      },
      "source": [
        "mod2ang_scores = evaluate(mod2ang_models, \r\n",
        "                          COMBINED_TEST_SRC, \r\n",
        "                          COMBINED_TEST_TGT, \r\n",
        "                          eval_metrics,\r\n",
        "                          token_kwargs,\r\n",
        "                          MAX_SENTENCE_LENGTH, \r\n",
        "                          5, \r\n",
        "                          str(MOD2ANG_PREDICTIONS_PATH))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoP0Dj9o5w9P"
      },
      "source": [
        "The best performing model is after 2900 training iterations with early stopping and beam size 5:\r\n",
        "\r\n",
        "    BLEU   = 10.9438\r\n",
        "    METEOR = 0.2551"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhqQLASVRDkI"
      },
      "source": [
        "## Modern to Modern"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sehy2zifYKC7"
      },
      "source": [
        "### KJV to BBE\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YP-xsP39YWYy"
      },
      "source": [
        "from pathlib import Path\r\n",
        "\r\n",
        "KJV2BBE_TRANSLATE_NAME = 'kjv2bbe'\r\n",
        "!mkdir -p '{KJV2BBE_TRANSLATE_NAME}'\r\n",
        "\r\n",
        "# PATH VARIABLES\r\n",
        "KJV2BBE_TRANSLATE_PATH = Path(KJV2BBE_TRANSLATE_NAME)\r\n",
        "KJV2BBE_RUN_PATH = KJV2BBE_TRANSLATE_PATH / 'run'\r\n",
        "!mkdir -p \"{KJV2BBE_RUN_PATH}\"\r\n",
        "\r\n",
        "# Dataset Variables\r\n",
        "KJV2BBE_SOURCE_VER = 't_kjv'\r\n",
        "KJV2BBE_SRC_LANG_CODE = 'eng'\r\n",
        "KJV2BBE_TARGET_VER = 't_bbe'\r\n",
        "KJV2BBE_TGT_LANG_CODE = 'eng'\r\n",
        "\r\n",
        "MAX_SENTENCE_LENGTH = 60\r\n",
        "\r\n",
        "# Dataset Paths\r\n",
        "DATA_PATH = Path('data/preprocessed')\r\n",
        "!mkdir -p \"{DATA_PATH}\"\r\n",
        "\r\n",
        "set_deterministic()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xb4WzO-NYlOi"
      },
      "source": [
        "# Generate splits and write to files\r\n",
        "versions = get_bible_versions_by_file_name([KJV2BBE_SOURCE_VER, KJV2BBE_TARGET_VER])\r\n",
        "\r\n",
        "datasets = create_datasets(versions, .82, \r\n",
        "                preprocess_operations = [preprocess_filter_num_words(MAX_SENTENCE_LENGTH),\r\n",
        "                                         preprocess_expand_contractions(),\r\n",
        "                                         preprocess_filter_num_sentences(),\r\n",
        "                ]);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2E96kdFYriB"
      },
      "source": [
        "KJV2BBE_SRC_EXT = KJV2BBE_SOURCE_VER[2:]\r\n",
        "KJV2BBE_TGT_EXT = KJV2BBE_TARGET_VER[2:]\r\n",
        "\r\n",
        "\r\n",
        "kjv2bbe_file_paths = {\r\n",
        "    'training' : (DATA_PATH / f'bible-train.{KJV2BBE_SRC_EXT}', DATA_PATH / f'bible-train.{KJV2BBE_TGT_EXT}'),\r\n",
        "    'validation' : (DATA_PATH / f'bible-valid.{KJV2BBE_SRC_EXT}', DATA_PATH / f'bible-valid.{KJV2BBE_TGT_EXT}'),\r\n",
        "    'test' : (DATA_PATH / f'bible-test.{KJV2BBE_SRC_EXT}', DATA_PATH / f'bible-test.{KJV2BBE_TGT_EXT}')\r\n",
        "    }\r\n",
        "\r\n",
        "token_kwargs = {\r\n",
        "    'case_markup': True\r\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yfuACDqYuQh"
      },
      "source": [
        "write_tokenized_dataset(datasets, KJV2BBE_SOURCE_VER, KJV2BBE_SRC_LANG_CODE, KJV2BBE_TARGET_VER, KJV2BBE_TGT_LANG_CODE, kjv2bbe_file_paths, token_kwargs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3yz7ijRYwuD"
      },
      "source": [
        "KJV2BBE_SRC_VOCAB_PATH = KJV2BBE_RUN_PATH / 'vocab.src'\r\n",
        "KJV2BBE_TGT_VOCAB_PATH = KJV2BBE_RUN_PATH / 'vocab.tgt'\r\n",
        "\r\n",
        "kjv2bbe_yaml = 'kjv2bbe.yaml'\r\n",
        "\r\n",
        "KJV2BBE_MODEL_PATH = KJV2BBE_RUN_PATH / 'models'\r\n",
        "KJV2BBE_MODEL_PREFIX = 'kjv2bbe'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpxcgalyY0tD"
      },
      "source": [
        "config =  f'''# {kjv2bbe_yaml}\r\n",
        "save_data: {KJV2BBE_RUN_PATH}\r\n",
        "\r\n",
        "### DATA PROPROCESSING ###\r\n",
        "## Where the vocab(s) will be written\r\n",
        "src_vocab: {KJV2BBE_SRC_VOCAB_PATH}\r\n",
        "tgt_vocab: {KJV2BBE_TGT_VOCAB_PATH}\r\n",
        "\r\n",
        "# Corpus opts:\r\n",
        "data:\r\n",
        "    corpus_1:\r\n",
        "        path_src: {kjv2bbe_file_paths['training'][0]}\r\n",
        "        path_tgt: {kjv2bbe_file_paths['training'][1]}\r\n",
        "        transforms: []\r\n",
        "        weight: 1\r\n",
        "    valid:\r\n",
        "        path_src: {kjv2bbe_file_paths['validation'][0]}\r\n",
        "        path_tgt: {kjv2bbe_file_paths['validation'][1]}\r\n",
        "        transforms: []\r\n",
        "\r\n",
        "## silently ignore empty lines in data\r\n",
        "skip_empty_level: silent\r\n",
        "\r\n",
        "### TRAINING ###\r\n",
        "## Where the model will be saved\r\n",
        "save_model: {KJV2BBE_MODEL_PATH / KJV2BBE_MODEL_PREFIX}\r\n",
        "save_checkpoint_steps: 1000\r\n",
        "average_decay: 0.0005\r\n",
        "seed: 1234\r\n",
        "report_every: 100\r\n",
        "train_steps: 100000\r\n",
        "valid_steps: 100\r\n",
        "early_stopping: 10\r\n",
        "# early_stopping_criteria: accuracy\r\n",
        "tensorboard: True\r\n",
        "tensorboard_log_dir: {KJV2BBE_RUN_PATH / 'logs'}\r\n",
        "\r\n",
        "# Batching\r\n",
        "world_size: 1\r\n",
        "gpu_ranks: [0]\r\n",
        "batch_size: 64\r\n",
        "valid_batch_size: 64\r\n",
        "batch_size_multiple: 1\r\n",
        "\r\n",
        "# Optimization\r\n",
        "model_dtype: \"fp32\"\r\n",
        "optim: \"adam\"\r\n",
        "learning_rate: 0.001\r\n",
        "\r\n",
        "# Model\r\n",
        "encoder_type: rnn\r\n",
        "decoder_type: rnn\r\n",
        "rnn_type: LSTM\r\n",
        "bidir_edges: True\r\n",
        "enc_layers: 2\r\n",
        "dec_layers: 2\r\n",
        "rnn_size: 512\r\n",
        "word_vec_size: 256\r\n",
        "dropout: 0.5\r\n",
        "attn_dropout: 0.3\r\n",
        "'''\r\n",
        "\r\n",
        "with open(CONFIG_PATH / kjv2bbe_yaml, \"w+\") as config_yaml:\r\n",
        "  config_yaml.write(config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XiOOKTvY3zI"
      },
      "source": [
        "build_and_train(CONFIG_PATH / kjv2bbe_yaml)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fhbd32tKY6ca"
      },
      "source": [
        "# retrieve the models\r\n",
        "kjv2bbe_models = [ KJV2BBE_MODEL_PATH / f for f in listdir(KJV2BBE_MODEL_PATH) if f.startswith(KJV2BBE_MODEL_PREFIX)]\r\n",
        "\r\n",
        "KJV2BBE_PREDICTIONS_PATH = KJV2BBE_RUN_PATH / 'predictions'\r\n",
        "!mkdir -p \"{KJV2BBE_PREDICTIONS_PATH}\"\r\n",
        "\r\n",
        "eval_metrics = ['sacrebleu', 'meteor']\r\n",
        "\r\n",
        "kjv2bbe_scores = evaluate(kjv2bbe_models, \r\n",
        "                          kjv2bbe_file_paths['test'][0], \r\n",
        "                          kjv2bbe_file_paths['test'][1], \r\n",
        "                          eval_metrics,\r\n",
        "                          token_kwargs,\r\n",
        "                          MAX_SENTENCE_LENGTH, \r\n",
        "                          5, \r\n",
        "                          str(KJV2BBE_PREDICTIONS_PATH))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFAtSw5N8DpE"
      },
      "source": [
        "The best performing model is after 4000 training iterations with early stopping and beam size 5:\r\n",
        "\r\n",
        "    BLEU   = 36.048\r\n",
        "    METEOR = 0.5451"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tl4ZN87d-Jex"
      },
      "source": [
        "#### User Studies Predictions\r\n",
        "\r\n",
        "Generate predictions for the user studies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a-CXJdbyCJ1"
      },
      "source": [
        "queries = ['In the beginning God created the heaven and the earth.', \r\n",
        "           \"And Adam called his wife's name Eve; because she was the mother of all living.\", \r\n",
        "           'And the LORD God called unto Adam, and said unto him, Where art thou?', \r\n",
        "           'And God remembered Noah, and every living thing, and all the cattle that was with him in the ark: and God made a wind to pass over the earth, and the waters assuaged;', \r\n",
        "           'And God looked upon the earth, and, behold, it was corrupt; for all flesh had corrupted his way upon the earth.', \r\n",
        "           'There went in two and two unto Noah into the ark, the male and the female, as God had commanded Noah.', \r\n",
        "           'And the waters decreased continually until the tenth month: in the tenth month, on the first day of the month, were the tops of the mountains seen.', \r\n",
        "           'And on the seventh day God ended his work which he had made; and he rested on the seventh day from all his work which he had made.', \r\n",
        "           'And the earth brought forth grass, and herb yielding seed after his kind, and the tree yielding fruit, whose seed was in itself, after his kind: and God saw that it was good.', \r\n",
        "           'And God did so that night: for it was dry upon the fleece only, and there was dew on all the ground.', \r\n",
        "           'Then said the trees unto the vine, Come thou, and reign over us.', \r\n",
        "           'Wherefore I have not sinned against thee, but thou doest me wrong to war against me: the LORD the Judge be judge this day between the children of Israel and the children of Ammon.', \r\n",
        "           'The labour of the foolish wearieth every one of them, because he knoweth not how to go to the city.', \r\n",
        "           'And if he trespass against thee seven times in a day, and seven times in a day turn again to thee, saying, I repent; thou shalt forgive him.',\r\n",
        "           'You only have I known of all the families of the earth: therefore I will punish you for all your iniquities.', \r\n",
        "           'And he is the head of the body, the church: who is the beginning, the firstborn from the dead; that in all things he might have the preeminence.', \r\n",
        "           'And the four beasts said, Amen. And the four and twenty elders fell down and worshipped him that liveth for ever and ever.', \r\n",
        "           'He hath also broken my teeth with gravel stones, he hath covered me with ashes.', \r\n",
        "           'If Satan also be divided against himself, how shall his kingdom stand? because ye say that I cast out devils through Beelzebub.', \r\n",
        "           'For men shall be lovers of their own selves, covetous, boasters, proud, blasphemers, disobedient to parents, unthankful, unholy,', \r\n",
        "           'And God said, Let there be lights in the firmament of the heaven to divide the day from the night; and let them be for signs, and for seasons, and for days, and years:', \r\n",
        "           'And Isaac trembled very exceedingly, and said, Who? where is he that hath taken venison, and brought it me, and I have eaten of all before thou camest, and have blessed him? yea, and he shall be blessed.', \r\n",
        "           'And the angel of the LORD called unto him out of heaven, and said, Abraham, Abraham: and he said, Here am I.', \r\n",
        "           'Thus they made a covenant at Beersheba: then Abimelech rose up, and Phichol the chief captain of his host, and they returned into the land of the Philistines.', \r\n",
        "           'And Abraham stretched forth his hand, and took the knife to slay his son.', \r\n",
        "           'So Abraham returned unto his young men, and they rose up and went together to Beersheba; and Abraham dwelt at Beersheba.', \r\n",
        "           'And you, being dead in your sins and the uncircumcision of your flesh, hath he quickened together with him, having forgiven you all trespasses;', \r\n",
        "           'Wives, submit yourselves unto your own husbands, as it is fit in the Lord.', \r\n",
        "           'But he that doeth wrong shall receive for the wrong which he hath done: and there is no respect of persons.', \r\n",
        "           \"Aristarchus my fellowprisoner saluteth you, and Marcus, sister's son to Barnabas, (touching whom ye received commandments: if he come unto you, receive him;)\", \r\n",
        "           'Now unto God and our Father be glory for ever and ever. Amen.'\r\n",
        "           ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3EFlddk9KXl"
      },
      "source": [
        "model = KJV2BBE_MODEL_PATH / f'{KJV2BBE_MODEL_PREFIX}_step_4600.pt'\r\n",
        "BBE_TEST_TOK = DATA_PATH / 'user-studies.eng'\r\n",
        "BBE_TEST_PRED = KJV2BBE_PREDICTIONS_PATH / 'bbe-text-pred.txt'\r\n",
        "\r\n",
        "with open(BBE_TEST_TOK, mode='w+', encoding='utf-8') as f:\r\n",
        "      eval_text = [l.rstrip('\\n') for l in f]\r\n",
        "      f.write('\\n'.join([\" \".join(tokenizer(l, 'enm', **token_kwargs)) for l in queries]))\r\n",
        "\r\n",
        "!onmt_translate -model \"{model}\" -src \"{BBE_TEST_TOK}\" -output \"{BBE_TEST_PRED}\" -min_length 1 -max_length \"{MAX_SENTENCE_LENGTH}\" -beam_size 5 -gpu 0 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxPNRubW9RST"
      },
      "source": [
        "tokenize = pyonmttok.Tokenizer(\"aggressive\", **token_kwargs)\r\n",
        "hypotheses = get_detokenized_file(BBE_TEST_PRED, tokenize)\r\n",
        "\r\n",
        "for hyp in hypotheses:\r\n",
        "    print(hyp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_x7_8W1-Dj6"
      },
      "source": [
        "### BBE to KJV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9eCtafP-Ro8"
      },
      "source": [
        "from pathlib import Path\r\n",
        "\r\n",
        "BBE2KJV_TRANSLATE_NAME = 'bbe2kjv'\r\n",
        "!mkdir -p '{BBE2KJV_TRANSLATE_NAME}'\r\n",
        "\r\n",
        "# PATH VARIABLES\r\n",
        "BBE2KJV_TRANSLATE_PATH = Path(BBE2KJV_TRANSLATE_NAME)\r\n",
        "BBE2KJV_RUN_PATH = BBE2KJV_TRANSLATE_PATH / 'run'\r\n",
        "!mkdir -p \"{BBE2KJV_RUN_PATH}\"\r\n",
        "\r\n",
        "# Dataset Variables\r\n",
        "BBE2KJV_SOURCE_VER = 't_bbe'\r\n",
        "BBE2KJV_SRC_LANG_CODE = 'eng'\r\n",
        "BBE2KJV_TARGET_VER = 't_kjv'\r\n",
        "BBE2KJV_TGT_LANG_CODE = 'eng'\r\n",
        "\r\n",
        "MAX_SENTENCE_LENGTH = 60\r\n",
        "\r\n",
        "# Dataset Paths\r\n",
        "DATA_PATH = Path('data/preprocessed')\r\n",
        "!mkdir -p \"{DATA_PATH}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WV_phJ5x-qUh"
      },
      "source": [
        "BBE2KJV_SRC_EXT = BBE2KJV_SOURCE_VER[2:]\r\n",
        "BBE2KJV_TGT_EXT = BBE2KJV_TARGET_VER[2:]\r\n",
        "\r\n",
        "bbe2kjv_file_paths = {\r\n",
        "    'training' : (DATA_PATH / f'bible-train.{BBE2KJV_SRC_EXT}', DATA_PATH / f'bible-train.{BBE2KJV_TGT_EXT}'),\r\n",
        "    'validation' : (DATA_PATH / f'bible-valid.{BBE2KJV_SRC_EXT}', DATA_PATH / f'bible-valid.{BBE2KJV_TGT_EXT}'),\r\n",
        "    'test' : (DATA_PATH / f'bible-test.{BBE2KJV_SRC_EXT}', DATA_PATH / f'bible-test.{BBE2KJV_TGT_EXT}')\r\n",
        "    }\r\n",
        "\r\n",
        "token_kwargs = {\r\n",
        "    'case_markup': True\r\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULDwKpdq-4pJ"
      },
      "source": [
        "BBE2KJV_SRC_VOCAB_PATH = BBE2KJV_RUN_PATH / 'vocab.src'\r\n",
        "BBE2KJV_TGT_VOCAB_PATH = BBE2KJV_RUN_PATH / 'vocab.tgt'\r\n",
        "\r\n",
        "bbe2kjv_yaml = 'bbe2kjv.yaml'\r\n",
        "\r\n",
        "BBE2KJV_MODEL_PATH = BBE2KJV_RUN_PATH / 'models'\r\n",
        "BBE2KJV_MODEL_PREFIX = 'bbe2kjv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeJ8tSGO_D7j"
      },
      "source": [
        "config =  f'''# {bbe2kjv_yaml}\r\n",
        "save_data: {BBE2KJV_RUN_PATH}\r\n",
        "\r\n",
        "### DATA PROPROCESSING ###\r\n",
        "## Where the vocab(s) will be written\r\n",
        "src_vocab: {BBE2KJV_SRC_VOCAB_PATH}\r\n",
        "tgt_vocab: {BBE2KJV_TGT_VOCAB_PATH}\r\n",
        "\r\n",
        "# Corpus opts:\r\n",
        "data:\r\n",
        "    corpus_1:\r\n",
        "        path_src: {bbe2kjv_file_paths['training'][0]}\r\n",
        "        path_tgt: {bbe2kjv_file_paths['training'][1]}\r\n",
        "        transforms: []\r\n",
        "        weight: 1\r\n",
        "    valid:\r\n",
        "        path_src: {bbe2kjv_file_paths['validation'][0]}\r\n",
        "        path_tgt: {bbe2kjv_file_paths['validation'][1]}\r\n",
        "        transforms: []\r\n",
        "\r\n",
        "## silently ignore empty lines in data\r\n",
        "skip_empty_level: silent\r\n",
        "\r\n",
        "### TRAINING ###\r\n",
        "## Where the model will be saved\r\n",
        "save_model: {BBE2KJV_MODEL_PATH / BBE2KJV_MODEL_PREFIX}\r\n",
        "save_checkpoint_steps: 1000\r\n",
        "average_decay: 0.0005\r\n",
        "seed: 1234\r\n",
        "report_every: 100\r\n",
        "train_steps: 100000\r\n",
        "valid_steps: 100\r\n",
        "early_stopping: 10\r\n",
        "# early_stopping_criteria: accuracy\r\n",
        "tensorboard: True\r\n",
        "tensorboard_log_dir: {BBE2KJV_RUN_PATH / 'logs'}\r\n",
        "\r\n",
        "# Batching\r\n",
        "world_size: 1\r\n",
        "gpu_ranks: [0]\r\n",
        "batch_size: 64\r\n",
        "valid_batch_size: 64\r\n",
        "batch_size_multiple: 1\r\n",
        "\r\n",
        "# Optimization\r\n",
        "model_dtype: \"fp32\"\r\n",
        "optim: \"adam\"\r\n",
        "learning_rate: 0.001\r\n",
        "\r\n",
        "# Model\r\n",
        "encoder_type: rnn\r\n",
        "decoder_type: rnn\r\n",
        "rnn_type: LSTM\r\n",
        "bidir_edges: True\r\n",
        "enc_layers: 2\r\n",
        "dec_layers: 2\r\n",
        "rnn_size: 512\r\n",
        "word_vec_size: 256\r\n",
        "dropout: 0.5\r\n",
        "attn_dropout: 0.3\r\n",
        "'''\r\n",
        "\r\n",
        "with open(CONFIG_PATH / bbe2kjv_yaml, \"w+\") as config_yaml:\r\n",
        "  config_yaml.write(config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Na-rD7WT_Pxd"
      },
      "source": [
        "build_and_train(CONFIG_PATH / bbe2kjv_yaml)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KES6Qf1L_Svq"
      },
      "source": [
        "# retrieve the models\r\n",
        "bbe2kjv_models = [ BBE2KJV_MODEL_PATH / f for f in listdir(BBE2KJV_MODEL_PATH) if f.startswith(BBE2KJV_MODEL_PREFIX)]\r\n",
        "\r\n",
        "BBE2KJV_PREDICTIONS_PATH = BBE2KJV_RUN_PATH / 'predictions'\r\n",
        "!mkdir -p \"{BBE2KJV_PREDICTIONS_PATH}\"\r\n",
        "\r\n",
        "eval_metrics = ['sacrebleu', 'meteor']\r\n",
        "\r\n",
        "bbe2kjv_scores = evaluate(bbe2kjv_models, \r\n",
        "                          bbe2kjv_file_paths['test'][0], \r\n",
        "                          bbe2kjv_file_paths['test'][1], \r\n",
        "                          eval_metrics,\r\n",
        "                          token_kwargs,\r\n",
        "                          MAX_SENTENCE_LENGTH, \r\n",
        "                          5, \r\n",
        "                          str(BBE2KJV_PREDICTIONS_PATH))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7FPLnCyENjb"
      },
      "source": [
        "The best performing model is after 4000 training iterations with early stopping and beam size 5:\r\n",
        "\r\n",
        "    BLEU   = 31.2598\r\n",
        "    METEOR = 0.4973"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVRlyBymEyTg"
      },
      "source": [
        "#### User Studies Predictions\r\n",
        "\r\n",
        "Generate predictions for the user studies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcgrCJeEDcKq"
      },
      "source": [
        "queries = ['At the first God made the heaven and the earth', \r\n",
        "           'And the man gave his wife the name of Eve because she was the mother of all who have life.', \r\n",
        "           'And the voice of the Lord God came to the man, saying, Where are you?', \r\n",
        "           'And God kept Noah in mind, and all the living things and the cattle which were with him in the ark: and God sent a wind over the earth, and the waters went down.', \r\n",
        "           'And God, looking on the earth, saw that it was evil: for the way of all flesh had become evil on the earth.', \r\n",
        "           'In twos, male and female, they went into the ark with Noah, as God had said.', \r\n",
        "           'And still the waters went on falling, till on the first day of the tenth month the tops of the mountains were seen.', \r\n",
        "           'And on the seventh day God came to the end of all his work; and on the seventh day he took his rest from all the work which he had done.', \r\n",
        "           'And grass came up on the earth, and every plant producing seed of its sort, and every tree producing fruit, in which is its seed, of its sort: and God saw that it was good.', \r\n",
        "           'And that night God did so; for the wool was dry, and there was dew on all the earth round it.', \r\n",
        "           'Then the trees said to the vine, You come and be king over us.', \r\n",
        "           'So I have done no wrong against you, but you are doing wrong to me in fighting against me: may the Lord, who is Judge this day, be judge between the children of Israel and the children of Ammon.', \r\n",
        "           'The work of the foolish will be a weariness to him, because he has no knowledge of the way to the town.', \r\n",
        "           'And if he does you wrong seven times in a day, and seven times comes to you and says, I have regret for what I have done; let him have forgiveness.', \r\n",
        "           'You only of all the families of the earth have I taken care of: for this reason I will send punishment on you for all your sins.', \r\n",
        "           'And he is the head of the body, the church: the starting point of all things, the first to come again from the dead; so that in all things he might have the chief place.', \r\n",
        "           'And the four beasts said, So be it. And the rulers went down on their faces and gave worship.', \r\n",
        "           'By him my teeth have been broken with crushed stones, and I am bent low in the dust.', \r\n",
        "           'If, then, Satan is at war with himself, how will he keep his kingdom? because you say that I send evil spirits out of men by the help of Beelzebul.', \r\n",
        "           'For men will be lovers of self, lovers of money, uplifted in pride, given to bitter words, going against the authority of their fathers, never giving praise, having no religion,', \r\n",
        "           'And God said, Let there be lights in the arch of heaven, for a division between the day and the night, and let them be for signs, and for marking the changes of the year, and for days and for years:', \r\n",
        "           'And in great fear Isaac said, Who then is he who got meat and put it before me, and I took it all before you came, and gave him a blessing, and his it will be?', \r\n",
        "           'But the voice of the angel of the Lord came from heaven, saying, Abraham, Abraham: and he said, Here am I.', \r\n",
        "           'So they made an agreement at Beer-sheba, and Abimelech and Phicol, the captain of his army, went back to the land of the Philistines.', \r\n",
        "           'And stretching out his hand, Abraham took the knife to put his son to death.', \r\n",
        "           'Then Abraham went back to his young men and they went together to Beer-sheba, the place where Abraham was living.', \r\n",
        "           'And you, being dead through your sins and the evil condition of your flesh, to you, I say, he gave life together with him, and forgiveness of all our sins;', \r\n",
        "           'Wives, be under the authority of your husbands, as is right in the Lord.', \r\n",
        "           \"For the wrongdoer will have punishment for the wrong he has done, without respect for any man's position.\", \r\n",
        "           'Aristarchus, my brother-prisoner, sends his love to you, and Mark, a relation of Barnabas (about whom you have been given orders: if he comes to you, be kind to him),', \r\n",
        "           'Now to God our Father be glory for ever and ever. So be it.'\r\n",
        "           ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TGmfXY3E1Ei"
      },
      "source": [
        "model = BBE2KJV_MODEL_PATH / f'{BBE2KJV_MODEL_PREFIX}_step_4600.pt'\r\n",
        "KJV_TEST_TOK = DATA_PATH / 'user-studies.kjv'\r\n",
        "KJV_TEST_PRED = BBE2KJV_PREDICTIONS_PATH / 'kjv-text-pred.txt'\r\n",
        "\r\n",
        "with open(KJV_TEST_TOK, mode='w+', encoding='utf-8') as f:\r\n",
        "      eval_text = [l.rstrip('\\n') for l in f]\r\n",
        "      f.write('\\n'.join([\" \".join(tokenizer(l, 'enm', **token_kwargs)) for l in queries]))\r\n",
        "\r\n",
        "!onmt_translate -model \"{model}\" -src \"{KJV_TEST_TOK}\" -output \"{KJV_TEST_PRED}\" -min_length 1 -max_length \"{MAX_SENTENCE_LENGTH}\" -beam_size 5 -gpu 0 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGVeL1p5E3Yl"
      },
      "source": [
        "tokenize = pyonmttok.Tokenizer(\"aggressive\", **token_kwargs)\r\n",
        "hypotheses = get_detokenized_file(KJV_TEST_PRED, tokenize)\r\n",
        "\r\n",
        "for hyp in hypotheses:\r\n",
        "    print(hyp)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}