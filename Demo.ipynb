{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Demo.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mayaschwarz/cs175--lfric-to-Albert/blob/main/Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aajs4nSKUCse"
      },
      "source": [
        "# Ælfric to Albert Demo Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h48qN4XWm_MX"
      },
      "source": [
        "This was designed to be run directly from Google Colab, your mileage may vary if run from elsewhere."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4-Fv4AlUCsm"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRMnk3EeULAG"
      },
      "source": [
        "This code will use a model and some helper files that are stored in a GitHub repository. The repository will be cloned temporarily."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gdlrsSBUjhx"
      },
      "source": [
        "!git clone https://github.com/mayaschwarz/cs175--lfric-to-Albert.git git/\n",
        "%cd git/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZShH8EeeUCsn"
      },
      "source": [
        "# This might take a few minutes\n",
        "!pip install texttable > /dev/null\n",
        "!pip install contractions > /dev/null\n",
        "!pip install transformers > /dev/null\n",
        "!pip install datasets==1.4.1 > /dev/null 2> /dev/null\n",
        "!pip install sentencepiece > /dev/null\n",
        "!pip install cltk==0.1.121 > /dev/null\n",
        "!pip install nltk==3.5 > /dev/null\n",
        "!pip install gdown > /dev/null\n",
        "!pip install pyyaml==5.3.1 > /dev/null\n",
        "!pip install torchvision==0.7.0 > /dev/null 2> /dev/null\n",
        "!pip install OpenNMT-py > /dev/null 2> /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UjkCEKqUCso"
      },
      "source": [
        "## A quick look at the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0yn-o9QUCso"
      },
      "source": [
        "One of the big limitations of our project was the limited corpus size. The bible corpus contains only around 30k verses, split between the Old and New Testaments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPEgAAAYUCsp"
      },
      "source": [
        "from summarize_data import *\n",
        "print_testament_table()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQ3cHbEnUCsr"
      },
      "source": [
        "Different books for the Bible can vary stylistically. They may be written in vastly different time periods, perspectives, and genres."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQbvo2kPUCss"
      },
      "source": [
        "print_genre_table()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2lbeAF2UCss"
      },
      "source": [
        "For accurate testing, it was important for us to get a roughly even spread of the different bible genres for our test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPP-3X42UCst"
      },
      "source": [
        "print_genre_data_split_table()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FWGRGooUCst"
      },
      "source": [
        "Different Modern and Middle English Bible versions may differ slightly in the exact verses provided, but are largely similar. On the other hand, the two Old English Bible versions we used, Aelfric's Old Testament and the West-Saxon Gospels, only contain a small subset of the total Bible books, let alone verses."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAfZ-ZMrUCsu"
      },
      "source": [
        "print_testament_table('t_alf')\n",
        "print()\n",
        "print_testament_table('t_wsg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfXEnr7MUCsu"
      },
      "source": [
        "While these two versions vary drastically, we combined their verses in order to use as much data as possible."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pVCH34wUCsv"
      },
      "source": [
        "print_genre_table('t_alf_wsg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g03jlMOFUCsv"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTOMrr2KUCsv"
      },
      "source": [
        "As shown above, different Bible versions can differ in which verses and books they contain. In order to train any sequence-to-sequence model, we first need to pair together all the Bible verses shared by the different relevant Bible versions. To do this, we use our `create_datasets` function. This function is our is our swiss army knife for data preprocessing. It does the following:\n",
        "\n",
        " - Pairs all Bible verses shared between the given Bible versions\n",
        " - Runs any number of specified text pre-processing operations\n",
        " - Sets aside the verses from pre-defined test books into a test set\n",
        " - Splits the remaining verses into training and validation sets depending on the requested training split\n",
        " - Saves the datasets to files if requested\n",
        " - Shuffles the datasets if requested\n",
        " - Returns the datasets in an easy-to-use dictionary format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7RmF7e9UCsw"
      },
      "source": [
        "from src.data_manager import *\n",
        "versions = get_bible_versions_by_file_name(['t_kjv', 't_bbe'])\n",
        "datasets = create_datasets(\n",
        "    bible_versions = versions,\n",
        "    training_fraction = 0.85,\n",
        "    preprocess_operations = [\n",
        "        preprocess_expand_contractions(),\n",
        "        preprocess_filter_num_words(max_num_words = 35, min_num_words = 4),\n",
        "        preprocess_filter_num_sentences(max_num_sentences = 1),\n",
        "        preprocess_remove_punctuation(preserve_periods = True),\n",
        "        preprocess_lowercase()\n",
        "    ],\n",
        "    write_files = True,\n",
        "    shuffle = False\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6mbgYGTUCsx"
      },
      "source": [
        "datasets['test']['t_kjv'][:3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwupsroHUCsx"
      },
      "source": [
        "Without any pre-process operations, the results would contain much more content:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csaDuJ7pUCsx"
      },
      "source": [
        "datasets = create_datasets(\n",
        "    bible_versions = versions,\n",
        "    training_fraction = 0.85,\n",
        "    write_files = True,\n",
        "    shuffle = False\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQSIHwPhUCsy"
      },
      "source": [
        "datasets['test']['t_kjv'][:4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzhTFV33UCsy"
      },
      "source": [
        "By saving the split datasets to files, the same data can be used for consistent results and repreducability. The data can be loaded quickly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByfVxeA9UCsy"
      },
      "source": [
        "!wc -l data/split/*\n",
        "print()\n",
        "datasets = load_datasets()\n",
        "datasets['test']['t_kjv'][:4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuhZF7nZUCsz"
      },
      "source": [
        "## Encoder-Decoder Model\n",
        "\n",
        "### Setting up the Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hD3onj1bRes"
      },
      "source": [
        "# standard library\n",
        "from argparse import Namespace\n",
        "\n",
        "import cltk\n",
        "import onmt\n",
        "import onmt.inputters\n",
        "import onmt.translate\n",
        "import onmt.model_builder\n",
        "from onmt.translate.translator import build_translator\n",
        "import pyonmttok\n",
        "import texttable\n",
        "import torch\n",
        "\n",
        "# local libraries\n",
        "from src.data_manager import *\n",
        "from src.paths import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVyvNoZMbRes"
      },
      "source": [
        "# Define hyperparameter constants across all the models for translation\n",
        "MIN_SENTENCE_LENGTH = 1\n",
        "MAX_SENTENCE_LENGTH = 60\n",
        "BEAM_SIZE = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqvHZA_obRes"
      },
      "source": [
        "#### Tokenization and Data Processing\n",
        "\n",
        "When training the models, we found that the models learned best, when it was given sentences that had been tokenized and replacing special characters with the common modern English equivalent (normalization, canonical form).\n",
        "\n",
        "**OLD ENGLISH**\n",
        "> Ðonne beoð swilce gedreccednyssa ... -> Donne beod swilce gedreccednyssa ...\n",
        "\n",
        "**MIDDLE ENGLISH**\n",
        "> as 3e lykeþ best -> as ye liketh best.\n",
        "\n",
        "The models also performed better (2-4% improvement in score) by encoding case information and punctation. We enable case encoding using the `case_markup` flag with `pyonmttok.Tokenizer`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SN5_i-MbRet"
      },
      "source": [
        "from cltk.corpus.middle_english.alphabet import normalize_middle_english\n",
        "from cltk.phonology.old_english.phonology import Word\n",
        "\n",
        "TOKENIZER = pyonmttok.Tokenizer(\"aggressive\", case_markup=True)\n",
        "\n",
        "def _normalize(text: str, language_code: str):\n",
        "    \"\"\"\n",
        "    Given the language code, applies appropriate data normalization to the text.\n",
        "    \"\"\"\n",
        "    if language_code == 'ang':\n",
        "        # old english\n",
        "        DONT_NORMALIZE = '!?.&,:;\"'\n",
        "        normalized_words = list()\n",
        "        for word in text.split():\n",
        "            if len(word) == 0:\n",
        "                continue\n",
        "\n",
        "            if word[-1] in DONT_NORMALIZE:\n",
        "                normalized_words.append(Word(word[:-1]).ascii_encoding() + word[-1])\n",
        "            else:\n",
        "                normalized_words.append(Word(word).ascii_encoding())\n",
        "\n",
        "        return ' '.join(normalized_words)\n",
        "    elif language_code == 'enm':\n",
        "        # middle english\n",
        "        return normalize_middle_english(text, to_lower=False, alpha_conv=True, punct=False)\n",
        "    return text\n",
        "\n",
        "def preprocess_data(data: [str], lang_code: str) -> [str]:\n",
        "    \"\"\"\n",
        "    Tokenizes a list of sentences and returns each as a space-separated token string.\n",
        "    Format is the preprocessing step before passing to the OpenNMT-py models.\n",
        "\n",
        "    Arguments:\n",
        "      data{[str]} -- list of sentences to be tokenized\n",
        "      lang_code{str} -- language code representing the data (eng, enm, ang), applies normalization\n",
        "\n",
        "    Returns:\n",
        "      [str] -- list of tokenized sentences\n",
        "    \"\"\"\n",
        "    return [' '.join(TOKENIZER.tokenize(_normalize(sent, lang_code))[0]) for sent in data]\n",
        "\n",
        "def postprocess_data(data: [str]) -> [str]:\n",
        "    \"\"\"\n",
        "    Detokenizes a list of space-separated token strings and returns each as a sentence.\n",
        "\n",
        "    Arguments:\n",
        "      data{[str]} -- list of list of space-separated token strings.\n",
        "\n",
        "    Returns:\n",
        "      [str] -- list of sentences\n",
        "    \"\"\"\n",
        "    return [TOKENIZER.detokenize(sent.split(' ')) for sent in data]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "064c0H4UbRet"
      },
      "source": [
        "#### Translation\n",
        "OpenNMT-py uses a translation script, `onmt_translate` that loads the model alongside translation parameters (beam_size, max length, etc.). \n",
        "\n",
        "We're going to use the python interface to build the translator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8eTJFlFbReu"
      },
      "source": [
        "from onmt.translate.translator import build_translator\n",
        "from argparse import Namespace\n",
        "\n",
        "def gen_model_translator(model_path: str) -> onmt.translate.Translator:\n",
        "    \"\"\"\n",
        "    Generates a translator object for an OpenNMT-py model, given the model path\n",
        "    and global translation parameters.\n",
        "\n",
        "    Arguments:\n",
        "      model_path {str} -- path to the model\n",
        "\n",
        "    Returns:\n",
        "      onmt.translate.Translator -- Wrapper class for translation of the given model\n",
        "    \"\"\"\n",
        "    opt = Namespace(fix_word_vecs_dec=False,\n",
        "                    fix_word_vecs_enc=False,\n",
        "                    alpha=0.0, \n",
        "                    ban_unk_token=False,\n",
        "                    batch_type='sents', \n",
        "                    beam_size=BEAM_SIZE, \n",
        "                    beta=-0.0, \n",
        "                    block_ngram_repeat=0, \n",
        "                    coverage_penalty='none', \n",
        "                    data_type='text', \n",
        "                    dump_beam='', \n",
        "                    fp32=False, \n",
        "                    gpu=-1, \n",
        "                    int8=False,\n",
        "                    ignore_when_blocking=[], \n",
        "                    length_penalty='none', \n",
        "                    max_length=MAX_SENTENCE_LENGTH, \n",
        "                    max_sent_length=None, \n",
        "                    min_length=MIN_SENTENCE_LENGTH, \n",
        "                    models=[model_path], \n",
        "                    n_best=1, \n",
        "                    output='/dev/null', \n",
        "                    phrase_table='', \n",
        "                    random_sampling_temp=1.0, \n",
        "                    random_sampling_topk=0, \n",
        "                    random_sampling_topp=0.0,\n",
        "                    ratio=-0.0, \n",
        "                    replace_unk=False, \n",
        "                    report_align=False, \n",
        "                    report_time=False, \n",
        "                    seed=829, \n",
        "                    stepwise_penalty=False, \n",
        "                    tgt=None, \n",
        "                    tgt_prefix=None,\n",
        "                    verbose=False)\n",
        "    return build_translator(opt, report_score=False)\n",
        "\n",
        "def translate(text: [str], lang_code: str, translator: onmt.translate.Translator) -> [str]:\n",
        "    \"\"\"\n",
        "    Performs batch level translation and returns detokenized strings\n",
        "    \"\"\"\n",
        "    x_tokenized = preprocess_data(text, lang_code)\n",
        "    hyp = translator.translate(x_tokenized, batch_size=len(x_tokenized))\n",
        "    return postprocess_data([h[0] for h in hyp[1]])\n",
        "\n",
        "def custom_sentence_wrapper(input_lang_code: str, translator: onmt.translate.Translator) -> None:\n",
        "    \"\"\"\n",
        "    Wrapper function to allow user to test input sentences.\n",
        "    \"\"\"\n",
        "    while True:\n",
        "        sent = input(\"Enter your own sentence here (or Q to quit): \")\n",
        "        if not sent:\n",
        "            continue\n",
        "        elif sent == 'Q':\n",
        "            break\n",
        "        else:\n",
        "            print('Translation: ', translate([sent], input_lang_code, translator)[0])\n",
        "            \n",
        "def format_output(predicted: [str], expected: [str]) -> str:\n",
        "    \"\"\"\n",
        "    Returns texttable string of predicted and expected values\n",
        "    \"\"\"\n",
        "    tableObj = texttable.Texttable() \n",
        "    tableObj.set_cols_align([\"c\", \"c\"]) \n",
        "    tableObj.set_cols_dtype([\"t\", \"t\"]) \n",
        "    tableObj.set_cols_valign([\"t\", \"t\"]) \n",
        "    tableObj.add_rows([[\"Expected\", \"Predicted\"], *zip(expected, predicted)])\n",
        "    return tableObj.draw()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmovbL8KbRev"
      },
      "source": [
        "### The Models\n",
        "\n",
        "Here we will showcase the best trained models for each translation direction.\n",
        "\n",
        "* Modern English <-> Modern English\n",
        "* Middle English <-> Modern English\n",
        "* Old English <-> Modern English\n",
        "\n",
        "Scores for BLEU and METEOR have been adjusted from decimals to percentiles for ease of reading."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LY-3Tx1DbRev"
      },
      "source": [
        "from pathlib import Path\n",
        "# download the best models\n",
        "!gdown --id 1dnkVQMfF72Qv-KUmNeAkJVyUSXsxh8ht --output onmt-models.zip\n",
        "!unzip onmt-models.zip\n",
        "\n",
        "# folder containing all best encoder-decoder models\n",
        "models = Path('onmt-models')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBaJ1V4kbRew"
      },
      "source": [
        "#### Modern English\n",
        "The Modern English models are models translating between Early Modern English used in the *King James Version (1611)* and Modern English used in the *Bible In Basic English (1965)*.\n",
        "\n",
        "These were our earliest models when figuring out how to build, tweak, and design using our custom Encoder-Decoder and OpenNMT. These models are also used in comparison against the Transformer Model.\n",
        "\n",
        "We will showcase the capabilities using verses from the *Book of Revelation*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIdTpVOtbRew"
      },
      "source": [
        "# get the book ids\n",
        "book_id = get_bible_book_id_map()\n",
        "\n",
        "# Specify which verses we want to recieve\n",
        "# VerseIdentifier(book, chapter, verse)\n",
        "ten_verses = [VerseIdentifier(book_id['revelation'], 6, i) for i in range(1,11)]\n",
        "\n",
        "# get the bible versions\n",
        "bbe_info, kjv_info = get_bible_versions_by_file_name(['t_bbe', 't_kjv'])\n",
        "bbe_bible = get_bible_verses(bbe_info)\n",
        "kjv_bible = get_bible_verses(kjv_info)\n",
        "\n",
        "# extract the verses selected\n",
        "bbe_verses = []\n",
        "kjv_verses = []\n",
        "for v in ten_verses:\n",
        "    bbe_verses.append(bbe_bible[v])\n",
        "    kjv_verses.append(kjv_bible[v])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOn-8ISIbRew"
      },
      "source": [
        "##### King James to Basic English\n",
        "\n",
        "This model is a 2-layer LSTM encoder-decoder with a bidirectional encoder, an hidden size of `256` and an embedding size of `256`. Dropout was used in both the hidden layers and attention mechanism, `0.5` and `0.3` respectfully.\n",
        "\n",
        "The best performing model was after 4000 training iterations with early stopping and beam size 5 with a score of `BLEU = 36.048` and `METEOR = 54.51`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPPcrM1CbRex"
      },
      "source": [
        "# the model path\n",
        "kjv2bbe = models / 'kjv2bbe_step_4000.pt'\n",
        "\n",
        "checkpoint = torch.load(kjv2bbe, map_location=lambda storage, loc: storage)\n",
        "\n",
        "# Need to set checkpoint flags (the training opts)\n",
        "checkpoint['opt'].fix_word_vecs_enc = False\n",
        "checkpoint['opt'].fix_word_vecs_dec = False\n",
        "\n",
        "torch.save(checkpoint, kjv2bbe)\n",
        "\n",
        "# generate the translator\n",
        "kjv2bbe_translator = gen_model_translator(kjv2bbe)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjnKKjuAbRex"
      },
      "source": [
        "hypothesis = translate(kjv_verses, 'eng', kjv2bbe_translator)\n",
        "print(format_output(hypothesis, bbe_verses))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uubmC5MAbRex"
      },
      "source": [
        "# run this cell to try out the model translation yourself!\n",
        "custom_sentence_wrapper('eng', kjv2bbe_translator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmV_VJQHbRex"
      },
      "source": [
        "##### Basic English to King James\n",
        "\n",
        "This model is a 2-layer LSTM encoder-decoder with a bidirectional encoder, an hidden size of `256` and an embedding size of `256`. Dropout was used in both the hidden layers and attention mechanism, `0.5` and `0.3` respectfully.\n",
        "\n",
        "The best performing model was after 4000 training iterations with early stopping and beam size 5 with a score of `BLEU = 31.26` and `METEOR = 49.73`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxJjhbY-bRex"
      },
      "source": [
        "# the model path\n",
        "bbe2kjv = models / 'bbe2kjv_step_4000.pt'\n",
        "\n",
        "checkpoint = torch.load(bbe2kjv, map_location=lambda storage, loc: storage)\n",
        "\n",
        "# Need to set checkpoint flags (the training opts)\n",
        "checkpoint['opt'].fix_word_vecs_enc = False\n",
        "checkpoint['opt'].fix_word_vecs_dec = False\n",
        "\n",
        "torch.save(checkpoint, bbe2kjv)\n",
        "\n",
        "# generate the translator\n",
        "bbe2kjv_translator = gen_model_translator(kjv2bbe)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ps7pqpiMbRey"
      },
      "source": [
        "hypothesis = translate(bbe_verses, 'eng', bbe2kjv_translator)\n",
        "print(format_output(hypothesis, bbe_verses))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bID2lB2gbRey"
      },
      "source": [
        "# run this cell to try out the model translation yourself!\n",
        "custom_sentence_wrapper('eng', bbe2kjv_translator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccHzh2HLbRey"
      },
      "source": [
        "#### Middle English\n",
        "\n",
        "The Middle English models are models translating between Middle English used in the *Wycliffe's Bible (1395?)* and Early Modern English used in the *King James Version (1611)*. King James was chosen over Bible in Basic English because it's vocabulary size was closer to our Middle English corpus and performed better in initial tests.\n",
        "\n",
        "We will showcase the capabilities using verses from the *Book of Revelation*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iG5HKW0UbRey"
      },
      "source": [
        "# get the book ids\n",
        "book_id = get_bible_book_id_map()\n",
        "\n",
        "# Specify which verses we want to recieve\n",
        "# VerseIdentifier(book, chapter, verse)\n",
        "ten_verses = [VerseIdentifier(book_id['revelation'], 6, i) for i in range(1,11)]\n",
        "\n",
        "# get the bible versions\n",
        "kjv_info, wyc_info = get_bible_versions_by_file_name(['t_kjv', 't_wyc'])\n",
        "kjv_bible = get_bible_verses(kjv_info)\n",
        "wyc_bible = get_bible_verses(wyc_info)\n",
        "\n",
        "# extract the verses selected\n",
        "kjv_verses = []\n",
        "wyc_verses = []\n",
        "for v in ten_verses:\n",
        "    kjv_verses.append(kjv_bible[v])\n",
        "    wyc_verses.append(wyc_bible[v])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8wWYFxZbRey"
      },
      "source": [
        "##### Wycliffe to King James\n",
        "\n",
        "This model is a 2-layer LSTM encoder-decoder with a bidirectional encoder, an hidden size of `1024` and an embedding size of `256`. Dropout was used in both the hidden layers and attention mechanism, `0.5` and `0.3` respectfully.\n",
        "\n",
        "The best performing model was after 2900 training iterations with early stopping and beam size 5 with a score of `BLEU = 26.56` and `METEOR = 44.81`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r69gLXKKbRey"
      },
      "source": [
        "# the model path\n",
        "enm2mod = models / 'enm2mod_step_2900.pt'\n",
        "\n",
        "checkpoint = torch.load(enm2mod, map_location=lambda storage, loc: storage)\n",
        "\n",
        "# Need to set checkpoint flags (the training opts)\n",
        "checkpoint['opt'].fix_word_vecs_enc = False\n",
        "checkpoint['opt'].fix_word_vecs_dec = False\n",
        "\n",
        "torch.save(checkpoint, enm2mod)\n",
        "\n",
        "# generate the translator\n",
        "enm2mod_translator = gen_model_translator(enm2mod)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "og1P04OwbRez"
      },
      "source": [
        "hypothesis = translate(wyc_verses, 'enm', enm2mod_translator)\n",
        "print(format_output(hypothesis, kjv_verses))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7GAlRt4bRez"
      },
      "source": [
        "# run this cell to try out the model translation yourself!\n",
        "custom_sentence_wrapper('enm', enm2mod_translator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h05ElceQbRez"
      },
      "source": [
        "##### King James to Wycliffe\n",
        "\n",
        "This model is a 2-layer LSTM encoder-decoder with a bidirectional encoder, an hidden size of `1024` and an embedding size of `256`. Dropout was used in both the hidden layers and attention mechanism, `0.5` and `0.3` respectfully.\n",
        "\n",
        "The best performing model was after 3000 training iterations with early stopping and beam size 5 with a score of `BLEU = 28.44` and `METEOR = 45.77`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKH0MoSLbRez"
      },
      "source": [
        "# the model path\n",
        "mod2enm = models / 'mod2enm_step_3000.pt'\n",
        "\n",
        "checkpoint = torch.load(mod2enm, map_location=lambda storage, loc: storage)\n",
        "\n",
        "# Need to set checkpoint flags (the training opts)\n",
        "checkpoint['opt'].fix_word_vecs_enc = False\n",
        "checkpoint['opt'].fix_word_vecs_dec = False\n",
        "\n",
        "torch.save(checkpoint, mod2enm)\n",
        "\n",
        "# generate the translator\n",
        "mod2enm_translator = gen_model_translator(mod2enm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NY9c9aCpbRez"
      },
      "source": [
        "hypothesis = translate(kjv_verses, 'enm', mod2enm_translator)\n",
        "print(format_output(hypothesis, wyc_verses))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZt6M99cbRez"
      },
      "source": [
        "# run this cell to try out the model translation yourself!\n",
        "custom_sentence_wrapper('eng', mod2enm_translator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZDm6yCNbRe0"
      },
      "source": [
        "#### Old English\n",
        "\n",
        "The Old English models are models translating between Middle English used in: \n",
        "* *Aelfric's Homilies of the Anglo-Saxon Church (993?)*\n",
        "* fragments of the *Old English Hexateuch (990-1010?)*\n",
        "* *West Saxon Gospels (990 to 1175?)*. \n",
        "\n",
        "Early Modern English used in the *King James Version (1611)*.\n",
        "\n",
        "We will showcase these capabilities using an excerpt from *Aelfric's Homilies*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIIdhPr4bRe0"
      },
      "source": [
        "old_sentences = [\n",
        "    'On þyssere andwerdan gelaðunge sind gemengde yfele and gode, swa swa clæne corn mid fulum coccele: ac on ende þyssere worulde se soða Dema hæt his englas gadrian þone coccel byrþenmælum, and awurpan into ðam unadwæscendlicum fyre.',\n",
        "    'Þa leas-gewitan ða lédon heora hacelan ætforan fotum sumes geonges cnihtes, se wæs geciged Saulus.',\n",
        "    'Þurh myrran is gehíwod cwelmbærnys ures flæsces; be ðam cweð seo halige gelaðung, \"Mine handa drypton myrran.\"',\n",
        "    'Næs seo eadige Maria na ofslegen ne gemartyrod lichomlice, ac gastlice.',\n",
        "    \"Saraí wæs his wíf gehaten, þæt is gereht, Min ealdor,ac God hi het syððan Sarra, þæt is, Ealdor,þæt heo nære synderlice hire hiredes ealdor geciged, ac forðrihte Ealdor'; þæt is to understandenne ealra gelyfedra wifa moder.\",\n",
        "    'Hwæt is ðis deadlice líf buton weg? Understandað nu hwilc sy on weges geswince to ateorigenne, and ðeah nelle þone weg geendigan.',\n",
        "    'His frynd sind engla heapas, forðan ðe hi healdað on heora staðelfæstnysse singallice his willan.',\n",
        "    'Ælc bisceop and ælc láreow is to hyrde gesett Godes folce, þæt hí sceolon þæt folc wið ðone wulf gescyldan.'\n",
        "]\n",
        "\n",
        "mod_sentences = [\n",
        "    'In this present church are mingled evil and good, as clean corn with foul cockle: but at the end of this world the true Judge will bid his angels gather the cockle by burthens, and cast it into the unquenchable fire.',\n",
        "    'The false witnesses then laid their coats before the feet of a young man who was called Saul.',\n",
        "    'By myrrh is typified the mortality of our flesh, concerning which the holy congregation says, \"My hands dropt myrrh.\"',\n",
        "    'The blessed Mary was not slain nor martyred bodily, but spiritually.',\n",
        "    \"His wife was called Sarai, which is interpreted, My chief; but God called her afterwards Sarah, that is Chief; that she might not be exclusively called her family's chief, but absolutely chief; which is to be understood, mother of all believing women.\",\n",
        "    'What is this deathlike life but a way? Understand now what it is to faint through the toil of the way, and yet not to desire the way to end.',\n",
        "    'His friends are companies of angels, because they in their steadfastness constantly observe his will.',\n",
        "    \"Every bishop and every teacher is placed as a shepherd over God's people, that they may shield the people against the wolf.\"\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qR3FZC3QbRe0"
      },
      "source": [
        "##### Old English Corpora to King James\n",
        "\n",
        "This model is a 2-layer LSTM encoder-decoder with a bidirectional encoder, an hidden size of `512` and an embedding size of `128`. Dropout was used in both the hidden layers and attention mechanism, `0.6` and `0.4` respectfully.\n",
        "\n",
        "The best performing model was after 3000 training iterations with early stopping and beam size 5 with a score of `BLEU = 16.99` and `METEOR = 33.38`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pasge-VZbRe0"
      },
      "source": [
        "# the model path\n",
        "ang2mod = models / 'ang2mod_step_3000.pt'\n",
        "\n",
        "checkpoint = torch.load(ang2mod, map_location=lambda storage, loc: storage)\n",
        "\n",
        "# Need to set checkpoint flags (the training opts)\n",
        "checkpoint['opt'].fix_word_vecs_enc = False\n",
        "checkpoint['opt'].fix_word_vecs_dec = False\n",
        "\n",
        "torch.save(checkpoint, ang2mod)\n",
        "\n",
        "# generate the translator\n",
        "ang2mod_translator = gen_model_translator(ang2mod)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AslHVlaZbRe0"
      },
      "source": [
        "hypothesis = translate(old_sentences, 'ang', ang2mod_translator)\n",
        "print(format_output(hypothesis, mod_sentences))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpWSsGqSbRe0"
      },
      "source": [
        "# run this cell to try out the model translation yourself!\n",
        "custom_sentence_wrapper('ang', ang2mod_translator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDCnpCCwbRe1"
      },
      "source": [
        "####  King James to Old English\n",
        "\n",
        "This model is a 2-layer LSTM encoder-decoder with a bidirectional encoder, an hidden size of `512` and an embedding size of `128`. Dropout was used in both the hidden layers and attention mechanism, `0.6` and `0.4` respectfully.\n",
        "\n",
        "The best performing model was after 2900 training iterations with early stopping and beam size 5 with a score of `BLEU = 10.94` and `METEOR = 25.51`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rc_oaSKmbRe1"
      },
      "source": [
        "# the model path\n",
        "mod2ang = models / 'mod2ang_step_2900.pt'\n",
        "\n",
        "checkpoint = torch.load(mod2ang, map_location=lambda storage, loc: storage)\n",
        "\n",
        "# Need to set checkpoint flags (the training opts)\n",
        "checkpoint['opt'].fix_word_vecs_enc = False\n",
        "checkpoint['opt'].fix_word_vecs_dec = False\n",
        "\n",
        "torch.save(checkpoint, mod2ang)\n",
        "\n",
        "# generate the translator\n",
        "mod2ang_translator = gen_model_translator(mod2ang)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3Szt9vAbRe1"
      },
      "source": [
        "hypothesis = translate(mod_sentences, 'eng', mod2ang_translator)\n",
        "print(format_output(hypothesis, old_sentences))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_WEx64mbRe1"
      },
      "source": [
        "# run this cell to try out the model translation yourself!\n",
        "custom_sentence_wrapper('eng', mod2ang_translator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bS6_ZEBJUCsz"
      },
      "source": [
        "## Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "km1n66knUCsz"
      },
      "source": [
        "# Import dependencies\n",
        "from transformers import (\n",
        "    BartForConditionalGeneration, BartTokenizer\n",
        ")\n",
        "\n",
        "model_path = 'bart-bbe-to-kjv-1615204968'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4C14s4MBUCs0"
      },
      "source": [
        "First, let's load one of our fine-tuned sequence-to-sequence transformer models along with a pre-trained tokenizer (this might take a minute):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-QtwvHeUCs0"
      },
      "source": [
        "# download and unzip the fine-tuned model\n",
        "!gdown --id 1Bx9o8Rt6MDItpG6uGSvJGEVuhXTXFqkq --output models.zip\n",
        "!unzip models.zip\n",
        "\n",
        "# load the fine-tuned model and the pre-trained tokenizer\n",
        "model = BartForConditionalGeneration.from_pretrained(model_path, max_length = 100)\n",
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ky_SdJQzUCs0"
      },
      "source": [
        "Next, let's define a transformer pipeline for translating text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVxMwjHvUCs1"
      },
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "translator = pipeline('translation_bbe_to_kjv', model = model, tokenizer = tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8-O7EUBUCs1"
      },
      "source": [
        "Finally, we can translate!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yt_AhrOzUCs1"
      },
      "source": [
        "num_verses = 4\n",
        "source_verses, target_verses = datasets['test']['t_bbe'][:num_verses], datasets['test']['t_kjv'][:num_verses]\n",
        "predicted_verses = [translation['translation_text'] for translation in translator(source_verses, return_text = True)]\n",
        "\n",
        "for (source_verse, target_verse, predicted_verse) in zip(source_verses, target_verses, predicted_verses):\n",
        "    print(f'SOURCE:    {source_verse}')\n",
        "    print(f'TARGET:    {target_verse}')\n",
        "    print(f'PREDICTED: {predicted_verse}')\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNlxPoZvUCs2"
      },
      "source": [
        "Best of all, you can make your own predictions (feel free to pass whatever you want to the translate function below!):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpwg9pmUUCs2"
      },
      "source": [
        "def translate(text: str) -> str:\n",
        "    # translates a single string\n",
        "    return translator(text, return_text = True)[0]['translation_text']\n",
        "\n",
        "translate('And the fearless instructors gave us a good grade in the class. For just they were. And full of kindness in their soul.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBcDgd2GUCs3"
      },
      "source": [
        "### Notable findings (Transformer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnokThIWUCs3"
      },
      "source": [
        "Feel free to skip this section if you're not interested."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBR7bq-mUCs3"
      },
      "source": [
        "The transformer seems to have learned parallelism:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-GvyDbLUCs3"
      },
      "source": [
        "print(translate('For they were just.'))\n",
        "print(translate('For they were just. And full of kindness in their soul.'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAkuyvu4UCs4"
      },
      "source": [
        "The structure of the second sentence was extrapolated into the first sentence, shown by how the first sentence was translated differently when followed by the second."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaR4Iw4RUCs4"
      },
      "source": [
        "The model learned that 'lord' is often capitalized in the King James Version:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26HmcmymUCs4"
      },
      "source": [
        "translate('What did the lord say to you?')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-q8V7obUCs5"
      },
      "source": [
        "The model may translate a sentence differently depending on the ending puncutation (compare word order with above):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pybsRnMgUCs6"
      },
      "source": [
        "translate('What did the lord say to you.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xVPzbl5UCs6"
      },
      "source": [
        "The model still learned to be derogatory towards homosexuals:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PB-TcBAUCs6"
      },
      "source": [
        "translate('He was a homosexual man.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1AjxKTiUCs7"
      },
      "source": [
        "It was, after all, trained from verses such as:\n",
        "`There shall be no prostitute of the daughters of Israel, neither shall there be a sodomite of the sons of Israel.`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8zKiBksUCs7"
      },
      "source": [
        "However, as opposed to our previous models, it seems like these later models with more training and slightly different methods were less biased against homosexuals and less prone to complete failure:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uDOEYpyUCs7"
      },
      "source": [
        "translate('He was a gay man.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxVur-y6UCs8"
      },
      "source": [
        "Previously: `He was a man of the offspring of the evil spirits;`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbWqPv4QUCs8"
      },
      "source": [
        "translate('The black man')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y37viX5RUCs8"
      },
      "source": [
        "Previously: `The black man, the king of the army, the captain of the army, the captains of the army, the captains of the captains of the captains...`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAxl__Y6UCs9"
      },
      "source": [
        "However, there was still gender bias, assuming that pretty much any profession is held by men, except those associated with women:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiwEL3_YUCs9"
      },
      "source": [
        "print(translate('The person had a marriage.'))\n",
        "print()\n",
        "print(translate('The carpenter had a marriage.'))\n",
        "print(translate('The tailor had a marriage.'))\n",
        "print(translate('The butcher had a marriage.'))\n",
        "print(translate('The blacksmith had a marriage.'))\n",
        "print(translate('The real estate agent had a marriage.'))\n",
        "print(translate('The journalist had a marriage.'))\n",
        "print(translate('The artist had a marriage.'))\n",
        "# etc., there are many more\n",
        "print()\n",
        "print(translate('The nurse had a marriage.'))\n",
        "print(translate('The babysitter had a marriage.'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2wWFHipUCs9"
      },
      "source": [
        "We received inconclusive results when trying to determine whether the gender bias was inherent to the models or if it was learned. More humerously, however:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeKW7IIQUCs-"
      },
      "source": [
        "translate('The avocado had a marriage.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSPeIlJAUCs-"
      },
      "source": [
        "The model understands context, and translates the same word differently even within the same sentence (loving -> loving and loving -> loveth):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXTChorrUCs-"
      },
      "source": [
        "translate(\"Now I'm saving all my loving for someone who's loving me\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDIlsKXhUCs_"
      },
      "source": [
        "Finally, some quotes by Yoda:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Y3xebWcUCs_"
      },
      "source": [
        "translate('Once you start down the dark path, forever will it dominate your destiny. Consume you, it will.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zu3TV6dCUCs_"
      },
      "source": [
        "translate('Death is a natural part of life. Rejoice for those around you who transform into the Force. Mourn them do not. Miss them do not. Attachment leads to jealously.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfBKaaiMUCs_"
      },
      "source": [
        "translate('On many long journeys have I gone. And waited, too, for others to return from journeys of their own. Some return; some are broken; some come back so different only their names remain.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvvXPqtlUCtA"
      },
      "source": [
        "translate('No longer certain, that one ever does win a war, I am. For in fighting the battles, the bloodshed, already lost we have. Yet, open to us a path remains. That unknown to the Sith is. Through this path, victory we may yet find. Not victory in the Clone Wars, but victory for all time.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fokvpFQUCtA"
      },
      "source": [
        "translate('I can’t believe it, said Luke Skywalker. And Yoda replied, That is why you fail.')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}