{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "modifiedPytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mayaschwarz/cs175--lfric-to-Albert/blob/main/modifiedPytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4faC0Sv3BW8"
      },
      "source": [
        "Modified Notebook to fit to bible dataset\r\n",
        "\r\n",
        "TODO:\r\n",
        "*   Download dataset\r\n",
        "*   Rewrite Encoder/Decoder RNN from GRU to LSTM (if possible)\r\n",
        "*   Run small sampleset and sav\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCXd-a3VQE6J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf937065-0085-4f79-c022-568a27deb71a"
      },
      "source": [
        "from google.colab import drive\r\n",
        "\r\n",
        "# default location for the drive\r\n",
        "ROOT = \"/content/drive\"\r\n",
        "\r\n",
        "drive.mount(ROOT) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxo8ufZPQgqQ"
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\r\n",
        "from io import open\r\n",
        "import unicodedata\r\n",
        "import string\r\n",
        "import re\r\n",
        "import random\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "from torch import optim\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2q9wfLWQ1hd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b7ca269-f736-42e4-a8ac-3627e8472776"
      },
      "source": [
        "# download the dataset from shared drive\r\n",
        "!curl -LO 'https://download.pytorch.org/tutorial/data.zip'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   451    0   451    0     0   1092      0 --:--:-- --:--:-- --:--:--  1092\n",
            "100 69886    0 69886    0     0   141k      0 --:--:-- --:--:-- --:--:--  141k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzedvQpwRaFq",
        "outputId": "503561cd-628f-4101-ac90-c735a8330e40"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data  drive  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDNwbeLVRDBr"
      },
      "source": [
        "!unzip data.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMYdDrIjRq8D"
      },
      "source": [
        "SOS_token = 0\r\n",
        "EOS_token = 1\r\n",
        "\r\n",
        "\r\n",
        "class Lang:\r\n",
        "    def __init__(self, name):\r\n",
        "        self.name = name\r\n",
        "        self.word2index = {}\r\n",
        "        self.word2count = {}\r\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\r\n",
        "        self.n_words = 2  # Count SOS and EOS\r\n",
        "\r\n",
        "    def addSentence(self, sentence):\r\n",
        "        for word in sentence.split(' '):\r\n",
        "            self.addWord(word)\r\n",
        "\r\n",
        "    def addWord(self, word):\r\n",
        "        if word not in self.word2index:\r\n",
        "            self.word2index[word] = self.n_words\r\n",
        "            self.word2count[word] = 1\r\n",
        "            self.index2word[self.n_words] = word\r\n",
        "            self.n_words += 1\r\n",
        "        else:\r\n",
        "            self.word2count[word] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ea_ZCIciRuRy"
      },
      "source": [
        "# Turn a Unicode string to plain ASCII, thanks to\r\n",
        "# https://stackoverflow.com/a/518232/2809427\r\n",
        "def unicodeToAscii(s):\r\n",
        "    return ''.join(\r\n",
        "        c for c in unicodedata.normalize('NFD', s)\r\n",
        "        if unicodedata.category(c) != 'Mn'\r\n",
        "    )\r\n",
        "\r\n",
        "# Lowercase, trim, and remove non-letter characters\r\n",
        "def normalizeString(s):\r\n",
        "    s = unicodeToAscii(s.lower().strip())\r\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\r\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\r\n",
        "    return s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lShUXIxRRv3G"
      },
      "source": [
        "def readLangs(lang1, lang2, reverse=False):\r\n",
        "    print(\"Reading lines...\")\r\n",
        "\r\n",
        "    # Read the file and split into lines\r\n",
        "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\r\n",
        "        read().strip().split('\\n')\r\n",
        "\r\n",
        "    # Split every line into pairs and normalize\r\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\r\n",
        "\r\n",
        "    # Reverse pairs, make Lang instances\r\n",
        "    if reverse:\r\n",
        "        pairs = [list(reversed(p)) for p in pairs]\r\n",
        "        input_lang = Lang(lang2)\r\n",
        "        output_lang = Lang(lang1)\r\n",
        "    else:\r\n",
        "        input_lang = Lang(lang1)\r\n",
        "        output_lang = Lang(lang2)\r\n",
        "\r\n",
        "    return input_lang, output_lang, pairs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpiHMCxjSAsp"
      },
      "source": [
        "MAX_LENGTH = 10\r\n",
        "\r\n",
        "eng_prefixes = (\r\n",
        "    \"i am \", \"i m \",\r\n",
        "    \"he is\", \"he s \",\r\n",
        "    \"she is\", \"she s \",\r\n",
        "    \"you are\", \"you re \",\r\n",
        "    \"we are\", \"we re \",\r\n",
        "    \"they are\", \"they re \"\r\n",
        ")\r\n",
        "\r\n",
        "\r\n",
        "def filterPair(p):\r\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\r\n",
        "        len(p[1].split(' ')) < MAX_LENGTH and \\\r\n",
        "        p[1].startswith(eng_prefixes)\r\n",
        "\r\n",
        "\r\n",
        "def filterPairs(pairs):\r\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZ3yLIBtSCZJ",
        "outputId": "66c1c76a-28fe-4481-8bc8-fb6e2d228a58"
      },
      "source": [
        "def prepareData(lang1, lang2, reverse=False):\r\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\r\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\r\n",
        "    pairs = filterPairs(pairs)\r\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\r\n",
        "    print(\"Counting words...\")\r\n",
        "    for pair in pairs:\r\n",
        "        input_lang.addSentence(pair[0])\r\n",
        "        output_lang.addSentence(pair[1])\r\n",
        "    print(\"Counted words:\")\r\n",
        "    print(input_lang.name, input_lang.n_words)\r\n",
        "    print(output_lang.name, output_lang.n_words)\r\n",
        "    return input_lang, output_lang, pairs\r\n",
        "\r\n",
        "\r\n",
        "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\r\n",
        "print(random.choice(pairs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Read 135842 sentence pairs\n",
            "Trimmed to 10599 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "fra 4345\n",
            "eng 2803\n",
            "['je suis fatiguee de boston .', 'i m tired of boston .']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJrGsqtTSFfb"
      },
      "source": [
        "# The Model\r\n",
        "class EncoderRNN(nn.Module):\r\n",
        "    def __init__(self, input_size, hidden_size):\r\n",
        "        super(EncoderRNN, self).__init__()\r\n",
        "        self.hidden_size = hidden_size\r\n",
        "\r\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\r\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\r\n",
        "\r\n",
        "    def forward(self, input, hidden):\r\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\r\n",
        "        output = embedded\r\n",
        "        output, hidden = self.gru(output, hidden)\r\n",
        "        return output, hidden\r\n",
        "\r\n",
        "    def initHidden(self):\r\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zj3SR0DDSiFO"
      },
      "source": [
        "class DecoderRNN(nn.Module):\r\n",
        "    def __init__(self, hidden_size, output_size):\r\n",
        "        super(DecoderRNN, self).__init__()\r\n",
        "        self.hidden_size = hidden_size\r\n",
        "\r\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\r\n",
        "        # Replaced the GRU layer with LSTM layer from tutorial\r\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\r\n",
        "        self.out = nn.Linear(hidden_size, output_size)\r\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\r\n",
        "\r\n",
        "    def forward(self, input, hidden):\r\n",
        "        output = self.embedding(input).view(1, 1, -1)\r\n",
        "        output = F.relu(output)\r\n",
        "        output, hidden = self.gru(output, hidden)\r\n",
        "        output = self.softmax(self.out(output[0]))\r\n",
        "        return output, hidden\r\n",
        "\r\n",
        "    def initHidden(self):\r\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VJk_WAnSkpE"
      },
      "source": [
        "class AttnDecoderRNN(nn.Module):\r\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\r\n",
        "        super(AttnDecoderRNN, self).__init__()\r\n",
        "        self.hidden_size = hidden_size\r\n",
        "        self.output_size = output_size\r\n",
        "        self.dropout_p = dropout_p\r\n",
        "        self.max_length = max_length\r\n",
        "\r\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\r\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\r\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\r\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\r\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\r\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\r\n",
        "\r\n",
        "    def forward(self, input, hidden, encoder_outputs):\r\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\r\n",
        "        embedded = self.dropout(embedded)\r\n",
        "\r\n",
        "        attn_weights = F.softmax(\r\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\r\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\r\n",
        "                                 encoder_outputs.unsqueeze(0))\r\n",
        "\r\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\r\n",
        "        output = self.attn_combine(output).unsqueeze(0)\r\n",
        "\r\n",
        "        output = F.relu(output)\r\n",
        "        output, hidden = self.gru(output, hidden)\r\n",
        "\r\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\r\n",
        "        return output, hidden, attn_weights\r\n",
        "\r\n",
        "    def initHidden(self):\r\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqwjJtZRSntM"
      },
      "source": [
        "def indexesFromSentence(lang, sentence):\r\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\r\n",
        "\r\n",
        "\r\n",
        "def tensorFromSentence(lang, sentence):\r\n",
        "    indexes = indexesFromSentence(lang, sentence)\r\n",
        "    indexes.append(EOS_token)\r\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\r\n",
        "\r\n",
        "\r\n",
        "def tensorsFromPair(pair):\r\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\r\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\r\n",
        "    return (input_tensor, target_tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "injw2GmhSpvh"
      },
      "source": [
        "teacher_forcing_ratio = 0.5\r\n",
        "\r\n",
        "\r\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\r\n",
        "    encoder_hidden = encoder.initHidden()\r\n",
        "\r\n",
        "    encoder_optimizer.zero_grad()\r\n",
        "    decoder_optimizer.zero_grad()\r\n",
        "\r\n",
        "    input_length = input_tensor.size(0)\r\n",
        "    target_length = target_tensor.size(0)\r\n",
        "\r\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\r\n",
        "\r\n",
        "    loss = 0\r\n",
        "\r\n",
        "    for ei in range(input_length):\r\n",
        "        encoder_output, encoder_hidden = encoder(\r\n",
        "            input_tensor[ei], encoder_hidden)\r\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\r\n",
        "\r\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\r\n",
        "\r\n",
        "    decoder_hidden = encoder_hidden\r\n",
        "\r\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\r\n",
        "\r\n",
        "    if use_teacher_forcing:\r\n",
        "        # Teacher forcing: Feed the target as the next input\r\n",
        "        for di in range(target_length):\r\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\r\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\r\n",
        "            loss += criterion(decoder_output, target_tensor[di])\r\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\r\n",
        "\r\n",
        "    else:\r\n",
        "        # Without teacher forcing: use its own predictions as the next input\r\n",
        "        for di in range(target_length):\r\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\r\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\r\n",
        "            topv, topi = decoder_output.topk(1)\r\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\r\n",
        "\r\n",
        "            loss += criterion(decoder_output, target_tensor[di])\r\n",
        "            if decoder_input.item() == EOS_token:\r\n",
        "                break\r\n",
        "\r\n",
        "    loss.backward()\r\n",
        "\r\n",
        "    encoder_optimizer.step()\r\n",
        "    decoder_optimizer.step()\r\n",
        "\r\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMo0lJVXSrL2"
      },
      "source": [
        "import time\r\n",
        "import math\r\n",
        "\r\n",
        "\r\n",
        "def asMinutes(s):\r\n",
        "    m = math.floor(s / 60)\r\n",
        "    s -= m * 60\r\n",
        "    return '%dm %ds' % (m, s)\r\n",
        "\r\n",
        "\r\n",
        "def timeSince(since, percent):\r\n",
        "    now = time.time()\r\n",
        "    s = now - since\r\n",
        "    es = s / (percent)\r\n",
        "    rs = es - s\r\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2gmnS5QSsom"
      },
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\r\n",
        "    start = time.time()\r\n",
        "    plot_losses = []\r\n",
        "    print_loss_total = 0  # Reset every print_every\r\n",
        "    plot_loss_total = 0  # Reset every plot_every\r\n",
        "\r\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\r\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\r\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs))\r\n",
        "                      for i in range(n_iters)]\r\n",
        "    criterion = nn.NLLLoss()\r\n",
        "\r\n",
        "    for iter in range(1, n_iters + 1):\r\n",
        "        training_pair = training_pairs[iter - 1]\r\n",
        "        input_tensor = training_pair[0]\r\n",
        "        target_tensor = training_pair[1]\r\n",
        "\r\n",
        "        loss = train(input_tensor, target_tensor, encoder,\r\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\r\n",
        "        print_loss_total += loss\r\n",
        "        plot_loss_total += loss\r\n",
        "\r\n",
        "        if iter % print_every == 0:\r\n",
        "            print_loss_avg = print_loss_total / print_every\r\n",
        "            print_loss_total = 0\r\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\r\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\r\n",
        "\r\n",
        "        if iter % plot_every == 0:\r\n",
        "            plot_loss_avg = plot_loss_total / plot_every\r\n",
        "            plot_losses.append(plot_loss_avg)\r\n",
        "            plot_loss_total = 0\r\n",
        "\r\n",
        "    showPlot(plot_losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tp4pkmreSt3g"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "plt.switch_backend('agg')\r\n",
        "import matplotlib.ticker as ticker\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "\r\n",
        "def showPlot(points):\r\n",
        "    plt.figure()\r\n",
        "    fig, ax = plt.subplots()\r\n",
        "    # this locator puts ticks at regular intervals\r\n",
        "    loc = ticker.MultipleLocator(base=0.2)\r\n",
        "    ax.yaxis.set_major_locator(loc)\r\n",
        "    plt.plot(points)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xmY6NS5SwBR"
      },
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\r\n",
        "    with torch.no_grad():\r\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\r\n",
        "        input_length = input_tensor.size()[0]\r\n",
        "        encoder_hidden = encoder.initHidden()\r\n",
        "\r\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\r\n",
        "\r\n",
        "        for ei in range(input_length):\r\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\r\n",
        "                                                     encoder_hidden)\r\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\r\n",
        "\r\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\r\n",
        "\r\n",
        "        decoder_hidden = encoder_hidden\r\n",
        "\r\n",
        "        decoded_words = []\r\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\r\n",
        "\r\n",
        "        for di in range(max_length):\r\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\r\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\r\n",
        "            decoder_attentions[di] = decoder_attention.data\r\n",
        "            topv, topi = decoder_output.data.topk(1)\r\n",
        "            if topi.item() == EOS_token:\r\n",
        "                decoded_words.append('<EOS>')\r\n",
        "                break\r\n",
        "            else:\r\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\r\n",
        "\r\n",
        "            decoder_input = topi.squeeze().detach()\r\n",
        "\r\n",
        "        return decoded_words, decoder_attentions[:di + 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmiHe-BbSx4Z"
      },
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\r\n",
        "    for i in range(n):\r\n",
        "        pair = random.choice(pairs)\r\n",
        "        print('>', pair[0])\r\n",
        "        print('=', pair[1])\r\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\r\n",
        "        output_sentence = ' '.join(output_words)\r\n",
        "        print('<', output_sentence)\r\n",
        "        print('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QD6iGAoCS0F2",
        "outputId": "9d525690-21dd-4283-846f-ed4cb469114f"
      },
      "source": [
        "hidden_size = 256\r\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\r\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\r\n",
        "\r\n",
        "trainIters(encoder1, attn_decoder1, 75000, print_every=5000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7m 7s (- 99m 48s) (5000 6%) 2.8742\n",
            "14m 20s (- 93m 15s) (10000 13%) 2.3022\n",
            "21m 37s (- 86m 30s) (15000 20%) 1.9879\n",
            "28m 50s (- 79m 20s) (20000 26%) 1.7314\n",
            "36m 2s (- 72m 4s) (25000 33%) 1.5398\n",
            "43m 20s (- 65m 0s) (30000 40%) 1.3943\n",
            "50m 32s (- 57m 45s) (35000 46%) 1.2225\n",
            "57m 49s (- 50m 35s) (40000 53%) 1.1073\n",
            "65m 9s (- 43m 26s) (45000 60%) 0.9860\n",
            "72m 33s (- 36m 16s) (50000 66%) 0.8957\n",
            "79m 52s (- 29m 2s) (55000 73%) 0.7919\n",
            "87m 11s (- 21m 47s) (60000 80%) 0.7418\n",
            "94m 41s (- 14m 34s) (65000 86%) 0.6665\n",
            "102m 15s (- 7m 18s) (70000 93%) 0.6006\n",
            "109m 40s (- 0m 0s) (75000 100%) 0.5575\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dojtz2yXS1gi",
        "outputId": "6226e33c-01da-44cf-e971-0a7b3de35ae5"
      },
      "source": [
        "evaluateRandomly(encoder1, attn_decoder1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> je plaisante .\n",
            "= i am joking .\n",
            "< i m kidding . <EOS>\n",
            "\n",
            "> ils ne sont pas mauvais .\n",
            "= they re not evil .\n",
            "< they re not evil . <EOS>\n",
            "\n",
            "> nous avons des problemes avec notre nouveau voisin .\n",
            "= we are having trouble with our new neighbor .\n",
            "< we are having trouble with our new neighbor . <EOS>\n",
            "\n",
            "> je n en ai pas encore fini .\n",
            "= i m not finished yet .\n",
            "< i m not done yet . <EOS>\n",
            "\n",
            "> ce ne sont pas des criminels .\n",
            "= they re not criminals .\n",
            "< they re not criminals . <EOS>\n",
            "\n",
            "> il a des ennuis .\n",
            "= he is in trouble .\n",
            "< he is in trouble . <EOS>\n",
            "\n",
            "> ses parents lui font confiance .\n",
            "= he is trusted by his parents .\n",
            "< he is pleased by his parents . <EOS>\n",
            "\n",
            "> tu es trop ivre pour conduire .\n",
            "= you re too drunk to drive .\n",
            "< you re too drunk to drive . <EOS>\n",
            "\n",
            "> au rapport .\n",
            "= i m reporting for duty .\n",
            "< i m coming . <EOS>\n",
            "\n",
            "> c est la fille de mes reves .\n",
            "= she s the girl of my dreams .\n",
            "< she is my girl my dreams . <EOS>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhXlXYCeS3K3",
        "outputId": "22d2f140-f345-4dd3-d9a2-431f2e07b275"
      },
      "source": [
        "output_words, attentions = evaluate(encoder1, attn_decoder1, \"je suis trop froid .\")\r\n",
        "plt.matshow(attentions.numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f3c04fd5748>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ReQt8N1AS8JV",
        "outputId": "a7d96834-e237-464b-b691-e1783356e31c"
      },
      "source": [
        "def showAttention(input_sentence, output_words, attentions):\r\n",
        "    # Set up figure with colorbar\r\n",
        "    fig = plt.figure()\r\n",
        "    ax = fig.add_subplot(111)\r\n",
        "    cax = ax.matshow(attentions.numpy(), cmap='bone')\r\n",
        "    fig.colorbar(cax)\r\n",
        "\r\n",
        "    # Set up axes\r\n",
        "    ax.set_xticklabels([''] + input_sentence.split(' ') +\r\n",
        "                       ['<EOS>'], rotation=90)\r\n",
        "    ax.set_yticklabels([''] + output_words)\r\n",
        "\r\n",
        "    # Show label at every tick\r\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\r\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\r\n",
        "\r\n",
        "    plt.show()\r\n",
        "\r\n",
        "\r\n",
        "def evaluateAndShowAttention(input_sentence):\r\n",
        "    output_words, attentions = evaluate(\r\n",
        "        encoder1, attn_decoder1, input_sentence)\r\n",
        "    print('input =', input_sentence)\r\n",
        "    print('output =', ' '.join(output_words))\r\n",
        "    showAttention(input_sentence, output_words, attentions)\r\n",
        "\r\n",
        "\r\n",
        "evaluateAndShowAttention(\"elle a cinq ans de moins que moi .\")\r\n",
        "\r\n",
        "evaluateAndShowAttention(\"elle est trop petit .\")\r\n",
        "\r\n",
        "evaluateAndShowAttention(\"je ne crains pas de mourir .\")\r\n",
        "\r\n",
        "evaluateAndShowAttention(\"c est un jeune directeur plein de talent .\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input = elle a cinq ans de moins que moi .\n",
            "output = she s five years younger than me . <EOS>\n",
            "input = elle est trop petit .\n",
            "output = she s too drunk . <EOS>\n",
            "input = je ne crains pas de mourir .\n",
            "output = i m not scared to die . <EOS>\n",
            "input = c est un jeune directeur plein de talent .\n",
            "output = he s a talented young director . <EOS>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4XHwP_QnoEg"
      },
      "source": [
        "# Save the models\r\n",
        "torch.save(encoder1.state_dict(), 'seq2seqTutEncoder.dict')\r\n",
        "torch.save(attn_decoder1.state_dict(), 'seq2seqTutDecoder.dict')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}